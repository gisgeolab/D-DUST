{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e1c351c",
   "metadata": {},
   "source": [
    "# Ground Sensors data request notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d050786",
   "metadata": {},
   "source": [
    "This notebook is used to download data from both meteorological and air quality ground sensors of the ARPA Lombardia network segment. These data are structured a bit differently and in this introduction, they are described to provide a better explanation for the two datasets.\n",
    "\n",
    "In detail, each sensor type, position, and time series are retrieved as follows:\n",
    "- The ground sensor's type and position are always retrieved through the ARPA API at the following links: [Air quality stations](https://www.dati.lombardia.it/Ambiente/Stazioni-qualit-dell-aria/ib47-atvt) and [Meteorological stations](https://www.dati.lombardia.it/Ambiente/Stazioni-Meteorologiche/nf78-nj6b)\n",
    "- Time series are available through API requests for the **current year only** (e.g. from January 2022 if the current year is 2022) for [Air quality data](https://www.dati.lombardia.it/Ambiente/Dati-sensori-aria/nicp-bhqi), while are available for the **current month** for  [Meteorological data](https://www.dati.lombardia.it/Ambiente/Dati-sensori-meteo/647i-nhxk) (this may change in the future)\n",
    "- To use data from previous years it's required to use the dataset in .csv format, such as [Air quality data for 2020]( https://www.dati.lombardia.it/Ambiente/Dati-sensori-aria-2020/88sp-5tmj) or [Meteorological data for 2020](https://www.dati.lombardia.it/Ambiente/Dati-sensori-meteo-2020/erjn-istm). It is possible to download the .zip folder containing these .csv data using the `DDUST_methods.py` contained inside the  `functions` folder. The links for downloading these data are contained inside the `AQ_sensor` and the `meteo_sensor` functions. One .zip folder for each year is available.\n",
    "\n",
    "The zipped file is downloaded only if not already available in the working directory, otherwise, the download is skipped.\n",
    "\n",
    "It's necessary to change the time range in the `date.json` file and the right data will be automatically considered.\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<span>&#9888;</span>\n",
    "<a id='warning'></a> Note: at the end of the notebook there is a function where ground sensor data interpolation is performed using Radial Basis Function (RBF) method (from Scipy (https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.Rbf.html) library) over the region of interest, in order to get continuous information of the air quality and meteorological data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613745f5",
   "metadata": {},
   "source": [
    "### Reference:\n",
    "ARPA data from API are accessible using [Socrata Open Data API](https://dev.socrata.com/). <br>\n",
    "\n",
    "Register on \"Open Data Lombardia\" to get the API token: https://www.dati.lombardia.it/login\n",
    "\n",
    "The library [sodapy](https://github.com/xmunoz/sodapy) is a python client for the Socrata Open Data API.\n",
    "The `app_token` is required to access the data. The token can be copied and pasted into the `keys.json` file. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43a976f",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feaa305",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sodapy import Socrata\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "import json\n",
    "import io\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Set current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Import functions defined for DDUST project:\n",
    "from functions import DDUST_methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489fe353",
   "metadata": {},
   "source": [
    "Modify **date.json** file to change the date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4ef9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = open('date.json')  # Import time range from the date.json file\n",
    "date = json.load(d)\n",
    "year = date['year']\n",
    "custom_week = date['custom_week']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e8acbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select start and end date of the corresponding selected week:\n",
    "start_date = datetime.datetime.strptime((str(year)+'-'+custom_week[0]), \"%Y-%m-%d\").date()\n",
    "end_date = datetime.datetime.strptime((str(year)+'-'+custom_week[1]), \"%Y-%m-%d\").date()\n",
    "print(\"The starting date is\", start_date,\"and the ending date is\" , end_date,\". The date is define as yyyy-mm-dd.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b394f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract start, end dates and selected year for ARPA sensor. This is done to consider also the last day in the computation, otherwise it's skipped\n",
    "start_date_dt = datetime.datetime.strptime((str(year)+'-'+custom_week[0]), '%Y-%m-%d')\n",
    "end_date_dt = datetime.datetime.strptime((str(year)+'-'+custom_week[1]), '%Y-%m-%d')+timedelta(days=1) #increase 1 day to select data from arpa sensor correctly\n",
    "start_date = str(start_date_dt)[0:10]\n",
    "end_date = str(end_date_dt)[0:10]\n",
    "year = datetime.datetime.strptime(start_date, '%Y-%m-%d').date().year\n",
    "\n",
    "# Key and app token for Socrata API\n",
    "f = open('keys.json')\n",
    "keys = json.load(f)\n",
    "\n",
    "#Z-Score threshold for removing outliers\n",
    "threshold = 4  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7646d207-3d76-41d7-a41a-ad15d0e68b52",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a2ef3a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import meteorological stations information and positions from ARPA API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f837e094",
   "metadata": {},
   "source": [
    "This part will require meteorological sensor information, types and positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8f1ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "arpa_domain = \"www.dati.lombardia.it\"  # Select domain\n",
    "m_st_descr = \"nf78-nj6b\" # Select dataset\n",
    "client = Socrata(arpa_domain, app_token = keys['arpa_token'])  #Initialize client\n",
    "results = client.get_all(m_st_descr)\n",
    "meteo_st_descr = pd.DataFrame(results)\n",
    "meteo_st_descr[\"idsensore\"] = meteo_st_descr[\"idsensore\"].astype(int)\n",
    "meteo_st_descr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e2542b-ab92-4fa3-a70e-d43db154d444",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0363cc",
   "metadata": {},
   "source": [
    "# Import Meteorological data from ARPA API (current month data) or .csv file (past years)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f5524c-7141-4ded-b533-3fa0eb6e6032",
   "metadata": {},
   "source": [
    "The next code is used to request data from API (**only current month for meteorological data**) or download the .zip folder containing the .csv files (past years data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a62881e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If current year is selected use data from API (only current months data ara available from meteo sensors)\n",
    "if int(year) == datetime.datetime.today().year:\n",
    "    \n",
    "    # Set domain and token \n",
    "    arpa_domain = \"www.dati.lombardia.it\"\n",
    "    dati = \"647i-nhxk\" #change this depending on the dataset (check Open Data Lombardia datasets)\n",
    "    \n",
    "    # Query the data\n",
    "    client = Socrata(arpa_domain, app_token = keys['arpa_token']) #insert your arpa_token\n",
    "    date_query = \"data > {} and data < {}\".format('\"'+ start_date + '\"','\"'+ end_date + '\"')\n",
    "    results = client.get(dati, where=date_query, limit=5000000000000)\n",
    "    \n",
    "    # Create the dataframe\n",
    "    meteo_data = pd.DataFrame(results)\n",
    "    meteo_data.rename(columns={'IdSensore': 'idsensore','Data': 'data','idOperatore': 'idoperatore','Stato': 'stato','Valore': 'valore'}, inplace=True)\n",
    "    meteo_data['data'] =  pd.to_datetime(meteo_data['data'], format='%Y/%m/%d %H:%M:%S')\n",
    "    meteo_data_df = meteo_data.astype({\"idsensore\": int,\"valore\": float})\n",
    "    \n",
    "# If previous years download the corresponding year .csv file and filter the dates  \n",
    "elif int(year) < datetime.datetime.today().year:\n",
    "    filename_meteo = 'meteo_'+str(year)+'.zip'\n",
    "    \n",
    "    #if file does not exist then download it\n",
    "    if not os.path.exists(os.path.join(filename_meteo)):\n",
    "        csv_url = my_methods.meteo_sensor(str(year))\n",
    "        r_meteo = requests.get(csv_url, allow_redirects=True)\n",
    "        DL_zip = open(filename, 'wb').write(r_meteo.content)\n",
    "        print('Dowloaded zip file')\n",
    "    \n",
    "    print('Zip file exist')\n",
    "    \n",
    "    # Open the zip file\n",
    "    archive = zipfile.ZipFile(filename_meteo, 'r')\n",
    "    data = archive.open(str(year)+'.csv') \n",
    "    \n",
    "    # Create dataframe\n",
    "    # Keep meteo_data_df in memory so is possible to use it for other time periods\n",
    "    meteo_data_df = pd.read_csv(data, dtype={\"IdSensore\": int,\"Valore\": float, \"Stato\": str, \"idOperatore\":str})\n",
    "    meteo_data_df.rename(columns={'IdSensore': 'idsensore','Data': 'data','idOperatore': 'idoperatore','Stato': 'stato','Valore': 'valore'}, inplace=True)\n",
    "    meteo_data_df['data'] =  pd.to_datetime(meteo_data_df['data'], format='%d/%m/%Y %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e209aab3-0892-410d-81a5-0c6c0c77e909",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mask meteo_data_df\n",
    "mask = (meteo_data_df.data >= start_date) & (meteo_data_df.data < end_date)\n",
    "meteo_data = meteo_data_df.loc[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7896ee3a",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b0b80f",
   "metadata": {},
   "source": [
    "# Meteorological data processing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799f6280",
   "metadata": {},
   "source": [
    "Drop `stato`, `idoperatore` columns and select valid data different from -9999:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbd41e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_data = meteo_data.drop(columns=['stato', 'idoperatore'])\n",
    "meteo_data = meteo_data[meteo_data.valore.astype(float) != -9999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fe7643-41e8-456c-9a10-7f89d77fbc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7666d5",
   "metadata": {},
   "source": [
    "Get sensors unique types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cb27bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_st_descr.tipologia.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba773355",
   "metadata": {},
   "source": [
    "Select sensors by adding them to the following list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c37e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_sensor_sel = ['Precipitazione','Temperatura','Umidità Relativa','Direzione Vento','Velocità Vento', 'Radiazione Globale']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95403cea",
   "metadata": {},
   "source": [
    "Join sensors data with their descriptions and information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c9d40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_table = pd.merge(meteo_data, meteo_st_descr, on = 'idsensore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b27716",
   "metadata": {},
   "source": [
    "Filter sensor by type using the `m_sensor_list`, checking if their type is in that list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91118ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_table['tipologia'].astype(str)\n",
    "meteo_table = meteo_table[meteo_table['tipologia'].isin(m_sensor_sel)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1d26c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_st = meteo_table.loc[meteo_table['tipologia'] == 'Temperatura']\n",
    "prec_st = meteo_table.loc[meteo_table['tipologia'] == 'Precipitazione']\n",
    "air_hum_st = meteo_table.loc[meteo_table['tipologia'] == 'Umidità Relativa']\n",
    "wind_dir_st = meteo_table.loc[meteo_table['tipologia'] == 'Direzione Vento']\n",
    "wind_speed_st = meteo_table.loc[meteo_table['tipologia'] == 'Velocità Vento']\n",
    "rad_glob_st = meteo_table.loc[meteo_table['tipologia'] == 'Radiazione Globale']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8494dcc-b648-46b2-acb4-35e0734fd80f",
   "metadata": {},
   "source": [
    "Since Wind Direction's values are expressed in Degrees North, the are divided into 8 categories:\n",
    " - `1` -> North: 0° - 22.5° / 337.5° - 360°\n",
    " - `2` -> North-East: 22.5° - 67.5°         \n",
    " - `3` -> East: 67.5° - 112.5°              \n",
    " - `4` -> South-East: 112.5° - 157.5°       \n",
    " - `5` -> South: 15.5° - 202.5°             \n",
    " - `6` -> South-West: 202.5° - 247.5°       \n",
    " - `7` -> West: 247.5° - 292.5°             \n",
    " - `8` -> North-West: 292.5° - 337.5°       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18062c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_dir_st = wind_dir_st.rename(columns={\"valore\": \"direzione\"})\n",
    "wind_dir_st.loc[(wind_dir_st.direzione >= 0 ) & (wind_dir_st.direzione < 22.5 ), 'valore'] = 1\n",
    "wind_dir_st.loc[(wind_dir_st.direzione > 337.5 ) & (wind_dir_st.direzione <= 360 ), 'valore'] = 1\n",
    "wind_dir_st.loc[(wind_dir_st.direzione >= 22.5 ) & (wind_dir_st.direzione < 67.5 ), 'valore'] = 2\n",
    "wind_dir_st.loc[(wind_dir_st.direzione >= 67.5 ) & (wind_dir_st.direzione < 112.5 ), 'valore'] = 3\n",
    "wind_dir_st.loc[(wind_dir_st.direzione >= 112.5 ) & (wind_dir_st.direzione < 157.5 ), 'valore'] = 4\n",
    "wind_dir_st.loc[(wind_dir_st.direzione >= 157.5 ) & (wind_dir_st.direzione < 202.5 ), 'valore'] = 5\n",
    "wind_dir_st.loc[(wind_dir_st.direzione >= 202.5 ) & (wind_dir_st.direzione < 247.5 ), 'valore'] = 6\n",
    "wind_dir_st.loc[(wind_dir_st.direzione >= 247.5 ) & (wind_dir_st.direzione < 292.5 ), 'valore'] = 7\n",
    "wind_dir_st.loc[(wind_dir_st.direzione >= 292.5 ) & (wind_dir_st.direzione <= 337.5 ), 'valore'] = 8\n",
    "wind_dir_st = wind_dir_st.groupby('idsensore')['valore'].apply(lambda x: x.mode().iat[0]).reset_index()\n",
    "wind_dir_st = pd.merge(wind_dir_st, meteo_st_descr, on = 'idsensore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda9dd1b-4512-4029-b94f-37d28b7990ee",
   "metadata": {},
   "source": [
    "To remove outliers from `precipitation` we consider a threshold of 100 mm/h (the values coming from the station are stored every 10 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a8ffa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# precipitation values less than 100 to remove outliers\n",
    "prec_st = prec_st[prec_st.valore < 100]\n",
    "prec_st = prec_st.groupby(['idsensore'],as_index=False).mean()\n",
    "prec_st = pd.merge(prec_st, meteo_st_descr, on = 'idsensore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a88f6ec",
   "metadata": {},
   "source": [
    "To remove outliers from other variables we can calculate the corresponding Z-Scores  and calculate mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbacd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_st['zscore'] = np.abs(stats.zscore(temp_st['valore'], nan_policy='propagate'))\n",
    "temp_st = temp_st[temp_st.zscore < threshold]\n",
    "temp_st = temp_st.groupby(['idsensore'],as_index=False).mean()\n",
    "temp_st = pd.merge(temp_st, meteo_st_descr, on = 'idsensore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82ab7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_hum_st['zscore'] = np.abs(stats.zscore(air_hum_st['valore'], nan_policy='propagate'))\n",
    "air_hum_st = air_hum_st[air_hum_st.zscore < threshold]\n",
    "air_hum_st = air_hum_st.groupby(['idsensore'],as_index=False).mean()\n",
    "air_hum_st = pd.merge(air_hum_st, meteo_st_descr, on = 'idsensore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6033ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rad_glob_st['zscore'] = np.abs(stats.zscore(rad_glob_st['valore'], nan_policy='propagate'))\n",
    "rad_glob_st = rad_glob_st[rad_glob_st.zscore < threshold]\n",
    "rad_glob_st = rad_glob_st.groupby(['idsensore'],as_index=False).mean()\n",
    "rad_glob_st = pd.merge(rad_glob_st, meteo_st_descr, on = 'idsensore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50e2dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_speed_st['zscore'] = np.abs(stats.zscore(wind_speed_st['valore'], nan_policy='propagate'))\n",
    "wind_speed_st = wind_speed_st[wind_speed_st.zscore < threshold]\n",
    "wind_speed_st = wind_speed_st.groupby(['idsensore'],as_index=False).mean()\n",
    "wind_speed_st = pd.merge(wind_speed_st, meteo_st_descr, on = 'idsensore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb40326d",
   "metadata": {},
   "source": [
    "Save sensors separately and create a `.gpkg` file for each one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879efd88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp_gdf = gpd.GeoDataFrame(temp_st, geometry=gpd.points_from_xy(temp_st.lng, temp_st.lat))\n",
    "temp_gdf = temp_gdf.set_crs('epsg:4326')\n",
    "prec_gdf = gpd.GeoDataFrame(prec_st, geometry=gpd.points_from_xy(prec_st.lng, prec_st.lat))\n",
    "prec_gdf = prec_gdf.set_crs('epsg:4326')\n",
    "air_hum_gdf = gpd.GeoDataFrame(air_hum_st, geometry=gpd.points_from_xy(air_hum_st.lng, air_hum_st.lat))\n",
    "air_hum_gdf = air_hum_gdf.set_crs('epsg:4326')\n",
    "wind_dir_gdf = gpd.GeoDataFrame(wind_dir_st, geometry=gpd.points_from_xy(wind_dir_st.lng, wind_dir_st.lat))\n",
    "wind_dir_gdf = wind_dir_gdf.set_crs('epsg:4326')\n",
    "wind_speed_gdf = gpd.GeoDataFrame(wind_speed_st, geometry=gpd.points_from_xy(wind_speed_st.lng, wind_speed_st.lat))\n",
    "wind_speed_gdf = wind_speed_gdf.set_crs('epsg:4326')\n",
    "rad_glob_gdf = gpd.GeoDataFrame(rad_glob_st, geometry=gpd.points_from_xy(rad_glob_st.lng, rad_glob_st.lat))\n",
    "rad_glob_gdf = rad_glob_gdf.set_crs('epsg:4326')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bee154c-7c6c-4d9a-af87-e4c00215aace",
   "metadata": {},
   "source": [
    "Save in the temporary `temp` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7280ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_gdf.to_file(cwd+\"/temp/temp_st.gpkg\", driver=\"GPKG\")\n",
    "prec_gdf.to_file(cwd+\"/temp/prec_st.gpkg\", driver=\"GPKG\")\n",
    "air_hum_gdf.to_file(cwd+\"/temp/air_hum_st.gpkg\", driver=\"GPKG\")\n",
    "wind_dir_gdf.to_file(cwd+\"/temp/wind_dir_st.gpkg\", driver=\"GPKG\")\n",
    "wind_speed_gdf.to_file(cwd+\"/temp/wind_speed_st.gpkg\", driver=\"GPKG\")\n",
    "rad_glob_gdf.to_file(cwd+\"/temp/rad_glob_st.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0f3b09",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc69c0f",
   "metadata": {},
   "source": [
    "# Import Air Quality stations information and positions from ARPA API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234d6fe9",
   "metadata": {},
   "source": [
    "Import air quality sensors descriptions and positions from the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada35c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "arpa_domain = \"www.dati.lombardia.it\"\n",
    "st_descr = \"ib47-atvt\"\n",
    "\n",
    "client = Socrata(arpa_domain, app_token = keys['arpa_token']) \n",
    "results = client.get_all(st_descr)\n",
    "\n",
    "air_st_descr = pd.DataFrame(results)\n",
    "air_st_descr[\"idsensore\"] = air_st_descr[\"idsensore\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cebc2c0",
   "metadata": {},
   "source": [
    "- - - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e463d807",
   "metadata": {},
   "source": [
    "# Import Air Quality data from ARPA API (current year data) or .csv file (past years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6bfcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If current year is selected use data from API\n",
    "if int(year) == datetime.datetime.today().year:\n",
    "    \n",
    "    # Set domain and token \n",
    "    arpa_domain = \"www.dati.lombardia.it\"\n",
    "    dati = \"nicp-bhqi\" #change this depending on the dataset (check Open Data Lombardia datasets)\n",
    "    client = Socrata(arpa_domain, app_token = keys['arpa_token'])\n",
    "    \n",
    "    # Query the data\n",
    "    date_query = \"data > {} and data < {}\".format('\"'+ start_date + '\"','\"'+ end_date + '\"')  #select date \n",
    "    results = client.get(dati, where=date_query, limit=5000000000)  # query\n",
    "    \n",
    "    # Create the dataframe\n",
    "    aq_data = pd.DataFrame(results) #create Dataframe\n",
    "    aq_data.rename(columns={'IdSensore': 'idsensore','Data': 'data','idOperatore': 'idoperatore','Stato': 'stato','Valore': 'valore'}, inplace=True)\n",
    "    aq_data['data'] =  pd.to_datetime(aq_data['data'], format='%Y/%m/%d %H:%M:%S')\n",
    "    aq_data_df = aq_data.astype({\"idsensore\": int,\"valore\": float})\n",
    "    \n",
    "# If previous years download the corresponding year .csv file and filter the dates  \n",
    "elif int(year) < datetime.datetime.today().year:\n",
    "    filename_aq = 'aq_'+str(year)+'.zip'\n",
    "    \n",
    "    #if file does not exist then download it\n",
    "    if not os.path.exists(os.path.join(filename_aq)):\n",
    "        csv_url = my_methods.AQ_sensor(str(year))\n",
    "        r_aq = requests.get(csv_url, allow_redirects=True)\n",
    "        DL_zip = open(filename_aq, 'wb').write(r_aq.content)\n",
    "        print('Dowloaded zip file')\n",
    "    \n",
    "    print('Zip file exist')\n",
    "    \n",
    "    # Open the zip file\n",
    "    archive = zipfile.ZipFile('aq_'+str(year)+'.zip', 'r')\n",
    "    data = archive.open(str(year)+'.csv') #open the zip\n",
    "    \n",
    "    # Create dataframe\n",
    "    aq_data_df=pd.read_csv(data)\n",
    "    aq_data_df.rename(columns={'IdSensore': 'idsensore','Data': 'data','idOperatore': 'idoperatore','Stato': 'stato','Valore': 'valore'}, inplace=True)\n",
    "    aq_data_df['data'] =  pd.to_datetime(aq_data_df['data'], format='%d/%m/%Y %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f89d15-fa09-4a99-aa6b-d6a338e8deb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (aq_data_df.data >= start_date) & (aq_data_df.data < end_date)\n",
    "aq_data = aq_data_df.loc[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1b1012",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe9e011",
   "metadata": {},
   "source": [
    "# Air quality data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9f68f9",
   "metadata": {},
   "source": [
    "Drop \"stato\" and \"idoperatore\" columns and select valid values different from -9999:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec42686",
   "metadata": {},
   "outputs": [],
   "source": [
    "aq_data = aq_data.drop(columns=['stato', 'idoperatore'])\n",
    "aq_data = aq_data[aq_data.valore.astype(float) != -9999]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66af98ca",
   "metadata": {},
   "source": [
    "Get the unique sensor type names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76ad095",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_st_descr.nometiposensore.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd53a61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_sel = ['Ossidi di Azoto', 'Monossido di Carbonio', 'Biossido di Azoto','Ozono',\n",
    "       'Biossido di Zolfo', 'Particelle sospese PM2.5','Ammoniaca','PM10 (SM2005)']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed5f022",
   "metadata": {},
   "source": [
    "Join sensors description and information with the mean value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081fd777",
   "metadata": {},
   "outputs": [],
   "source": [
    "aq_table = pd.merge(aq_data, air_st_descr, on='idsensore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b38e5b",
   "metadata": {},
   "source": [
    "Select sensors adding their names to the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4d1d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "aq_table['nometiposensore'].astype(str)\n",
    "aq_table = aq_table[aq_table['nometiposensore'].isin(sensor_sel)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a9cde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm25_st = aq_table.loc[aq_table['nometiposensore'] == 'Particelle sospese PM2.5']\n",
    "co_st = aq_table.loc[aq_table['nometiposensore'] == 'Monossido di Carbonio']\n",
    "no2_st = aq_table.loc[aq_table['nometiposensore'] == 'Biossido di Azoto']\n",
    "so2_st = aq_table.loc[aq_table['nometiposensore'] == 'Biossido di Zolfo']\n",
    "nh3_st = aq_table.loc[aq_table['nometiposensore'] == 'Ammoniaca']\n",
    "nox_st = aq_table.loc[aq_table['nometiposensore'] == 'Ossidi di Azoto']\n",
    "pm10_st = aq_table.loc[aq_table['nometiposensore'] == 'PM10 (SM2005)']\n",
    "o3_st = aq_table.loc[aq_table['nometiposensore'] == 'Ozono']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3e87c3",
   "metadata": {},
   "source": [
    "Z-Test to remove outliers and calculate mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f1f03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm25_st['zscore'] = np.abs(stats.zscore(pm25_st['valore'], nan_policy='propagate'))\n",
    "pm25_st = pm25_st[pm25_st.zscore < threshold]\n",
    "pm25_st = pm25_st.groupby(['idsensore'],as_index=False).mean()\n",
    "pm25_st = pd.merge(pm25_st, air_st_descr, on='idsensore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d74657",
   "metadata": {},
   "outputs": [],
   "source": [
    "co_st['zscore'] = np.abs(stats.zscore(co_st['valore'], nan_policy='propagate'))\n",
    "co_st = co_st[co_st.zscore < threshold]\n",
    "co_st = co_st.groupby(['idsensore'],as_index=False).mean()\n",
    "co_st = pd.merge(co_st, air_st_descr, on='idsensore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0fd8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "no2_st['zscore'] = np.abs(stats.zscore(no2_st['valore'], nan_policy='propagate'))\n",
    "no2_st = no2_st[no2_st.zscore < threshold]\n",
    "no2_st = no2_st.groupby(['idsensore'],as_index=False).mean()\n",
    "no2_st = pd.merge(no2_st, air_st_descr, on='idsensore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e77d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "so2_st['zscore'] = np.abs(stats.zscore(so2_st['valore'], nan_policy='propagate'))\n",
    "so2_st = so2_st[so2_st.zscore < threshold]\n",
    "so2_st = so2_st.groupby(['idsensore'],as_index=False).mean()\n",
    "so2_st = pd.merge(so2_st, air_st_descr, on='idsensore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da90f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "nh3_st['zscore'] = np.abs(stats.zscore(nh3_st['valore'], nan_policy='propagate'))\n",
    "nh3_st = nh3_st[nh3_st.zscore < threshold]\n",
    "nh3_st = nh3_st.groupby(['idsensore'],as_index=False).mean()\n",
    "nh3_st = pd.merge(nh3_st, air_st_descr, on='idsensore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240c4e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "nox_st['zscore'] = np.abs(stats.zscore(nox_st['valore'], nan_policy='propagate'))\n",
    "nox_st = nox_st[nox_st.zscore < threshold]\n",
    "nox_st = nox_st.groupby(['idsensore'],as_index=False).mean()\n",
    "nox_st = pd.merge(nox_st, air_st_descr, on='idsensore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b1b010",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm10_st['zscore'] = np.abs(stats.zscore(pm10_st['valore'], nan_policy='propagate'))\n",
    "pm10_st = pm10_st[pm10_st.zscore < threshold]\n",
    "pm10_st = pm10_st.groupby(['idsensore'],as_index=False).mean()\n",
    "pm10_st = pd.merge(pm10_st, air_st_descr, on='idsensore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2500d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3_st['zscore'] = np.abs(stats.zscore(o3_st['valore'], nan_policy='propagate'))\n",
    "o3_st = o3_st[o3_st.zscore < threshold]\n",
    "o3_st = o3_st.groupby(['idsensore'],as_index=False).mean()\n",
    "o3_st = pd.merge(o3_st, air_st_descr, on='idsensore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f305ba7c",
   "metadata": {},
   "source": [
    "Save sensors separately and create a .gpkg file for each one. They contain the mean value for that given variable calculated over the defined time range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c1a145",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm25_gdf = gpd.GeoDataFrame(pm25_st, geometry=gpd.points_from_xy(pm25_st.lng, pm25_st.lat))\n",
    "pm25_gdf = pm25_gdf.set_crs('epsg:4326')\n",
    "co_gdf = gpd.GeoDataFrame(co_st, geometry=gpd.points_from_xy(co_st.lng, co_st.lat))\n",
    "co_gdf = co_gdf.set_crs('epsg:4326')\n",
    "no2_gdf = gpd.GeoDataFrame(no2_st, geometry=gpd.points_from_xy(no2_st.lng, no2_st.lat))\n",
    "no2_gdf = no2_gdf.set_crs('epsg:4326')\n",
    "so2_gdf = gpd.GeoDataFrame(so2_st, geometry=gpd.points_from_xy(so2_st.lng, so2_st.lat))\n",
    "so2_gdf = so2_gdf.set_crs('epsg:4326')\n",
    "nh3_gdf = gpd.GeoDataFrame(nh3_st, geometry=gpd.points_from_xy(nh3_st.lng, nh3_st.lat))\n",
    "nh3_gdf = nh3_gdf.set_crs('epsg:4326')\n",
    "nox_gdf = gpd.GeoDataFrame(nox_st, geometry=gpd.points_from_xy(nox_st.lng, nox_st.lat))\n",
    "nox_gdf = nox_gdf.set_crs('epsg:4326')\n",
    "pm10_gdf = gpd.GeoDataFrame(pm10_st, geometry=gpd.points_from_xy(pm10_st.lng, pm10_st.lat))\n",
    "pm10_gdf = pm10_gdf.set_crs('epsg:4326')\n",
    "o3_gdf = gpd.GeoDataFrame(o3_st, geometry=gpd.points_from_xy(o3_st.lng, o3_st.lat))\n",
    "o3_gdf = o3_gdf.set_crs('epsg:4326')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d21aca7",
   "metadata": {},
   "source": [
    "Save in a **temp** folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b31e1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm25_gdf.to_file(cwd+\"/temp/pm25_st.gpkg\", driver=\"GPKG\")\n",
    "co_gdf.to_file(cwd+\"/temp/co_st.gpkg\", driver=\"GPKG\")\n",
    "no2_gdf.to_file(cwd+\"/temp/no2_st.gpkg\", driver=\"GPKG\")\n",
    "so2_gdf.to_file(cwd+\"/temp/so2_st.gpkg\", driver=\"GPKG\")\n",
    "nh3_gdf.to_file(cwd+\"/temp/nh3_st.gpkg\", driver=\"GPKG\")\n",
    "nox_gdf.to_file(cwd+\"/temp/nox_st.gpkg\", driver=\"GPKG\")\n",
    "pm10_gdf.to_file(cwd+\"/temp/pm10_st.gpkg\", driver=\"GPKG\")\n",
    "o3_gdf.to_file(cwd+\"/temp/o3_st.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9410221c-7f99-4082-8181-3f2208144be3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eb6f3c",
   "metadata": {},
   "source": [
    "# Sensor interpolation using Radial Basis Functions (RBF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991423c7-f4f3-4e95-a425-fac0ff52ff5e",
   "metadata": {},
   "source": [
    "ARPA sensors are interpolated using Scipy Radial Basis Functions (RBF) (this is not dont for ESA Air Quality Platforms they are few)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650b7616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal\n",
    "from osgeo import ogr\n",
    "import rasterio as rio\n",
    "from rasterio.transform import Affine\n",
    "from rasterio.crs import CRS\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import Rbf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bcca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm25_gdf = pm25_gdf.to_crs('epsg:32632')\n",
    "co_gdf = co_gdf.to_crs('epsg:32632')\n",
    "no2_gdf = no2_gdf.to_crs('epsg:32632')\n",
    "so2_gdf = so2_gdf.to_crs('epsg:32632')\n",
    "nh3_gdf = nh3_gdf.to_crs('epsg:32632')\n",
    "nox_gdf = nox_gdf.to_crs('epsg:32632')\n",
    "pm10_gdf = pm10_gdf.to_crs('epsg:32632')\n",
    "o3_gdf = o3_gdf.to_crs('epsg:32632')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cf1ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_gdf = temp_gdf.to_crs('epsg:32632')\n",
    "prec_gdf = prec_gdf.to_crs('epsg:32632')\n",
    "air_hum_gdf = air_hum_gdf.to_crs('epsg:32632')\n",
    "wind_dir_gdf = wind_dir_gdf.to_crs('epsg:32632')\n",
    "wind_speed_gdf = wind_speed_gdf.to_crs('epsg:32632')\n",
    "rad_glob_gdf = rad_glob_gdf.to_crs('epsg:32632')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab7b8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = gdal.Open(cwd+\"/bounding_box/bounding_box_buffer20_raster_32632.tif\")  #use a predefined raster to set dimensions\n",
    "gt = bb.GetGeoTransform()\n",
    "ulx = gt[0]\n",
    "uly = gt[3]\n",
    "resx = gt[1]\n",
    "resy = gt[5]\n",
    "xsize = bb.RasterXSize\n",
    "ysize = bb.RasterYSize\n",
    "lrx = ulx + xsize * resx\n",
    "lry = uly + ysize * resy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d0a8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "xsize, ysize  #image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f68fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ulx, uly #upper left coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3f6ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrx, lry #lower right coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c5d618-fa28-49fd-ad3f-aea05b86cc2a",
   "metadata": {},
   "source": [
    "Define the output desired spatial resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37b0a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "rRes = 250  #Spatial resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69121f18-375a-4441-b83c-a0148a6fa1bc",
   "metadata": {},
   "source": [
    "Create a meshgrid with given dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed678fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "xRange = np.arange(ulx,lrx+rRes,rRes)\n",
    "yRange = np.arange(lry,uly+rRes,rRes)\n",
    "gridX,gridY = np.meshgrid(xRange, yRange)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1a62a5-8c97-4edf-b7d8-15558dad26da",
   "metadata": {},
   "source": [
    "Create dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56130a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "aq_dict = {'pm25_gdf':pm25_gdf, 'co_gdf':co_gdf, 'no2_gdf':no2_gdf, 'so2_gdf':so2_gdf,\n",
    "           'nh3_gdf':nh3_gdf, 'nox_gdf':nox_gdf, 'pm10_gdf':pm10_gdf, 'o3_gdf':o3_gdf}\n",
    "\n",
    "meteo_dict = {'temp_gdf':temp_gdf, 'prec_gdf':prec_gdf, 'air_hum_gdf':air_hum_gdf,\n",
    "           'wind_speed_gdf':wind_speed_gdf, 'rad_glob_gdf':rad_glob_gdf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da672f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in aq_dict:\n",
    "    east = aq_dict[key].geometry.x\n",
    "    north = aq_dict[key].geometry.y\n",
    "    value = aq_dict[key][['valore']]\n",
    "    \n",
    "    rbf = Rbf(east, north, value, function='linear')\n",
    "    z_new = rbf(gridX.ravel(), gridY.ravel()).reshape(gridX.shape)\n",
    "    print(key)\n",
    "    plt.pcolor(gridX, gridY, z_new);\n",
    "    plt.plot(east, north, 'o', markerfacecolor='blue');\n",
    "    plt.xlabel('East'); plt.ylabel('North');\n",
    "    plt.title('RBF Linear interpolation');\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "    transform = Affine.translation(gridX[0][0]-rRes/2, gridY[0][0]-rRes/2)*Affine.scale(rRes,rRes)\n",
    "\n",
    "    new_dataset = rio.open(cwd +'/temp/aq_rbf_'+key[:(len(key)-4)]+'.tif', 'w', driver='GTiff',\n",
    "                            height = z_new.shape[0], width = z_new.shape[1],\n",
    "                            count=1, dtype=str(z_new.dtype),\n",
    "                            crs='+proj=utm +zone=32 +datum=WGS84 +units=m +no_defs',\n",
    "                            transform=transform)\n",
    "\n",
    "    new_dataset.write(z_new, 1)\n",
    "    new_dataset.close()\n",
    "    #Warp with GDAL since rasterio and rasterstats affine transformation don't work in the grid processing notebook\n",
    "    ds = gdal.Warp(cwd +'/temp/int_'+key[:(len(key)-4)]+'.tif', cwd +'/temp/aq_rbf_'+key[:(len(key)-4)]+'.tif', dstSRS='EPSG:32632',\n",
    "                   outputType=gdal.GDT_Float64, xRes=rRes, yRes=rRes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc72b5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in meteo_dict:\n",
    "    east = meteo_dict[key].geometry.x\n",
    "    north = meteo_dict[key].geometry.y\n",
    "    value = meteo_dict[key][['valore']]\n",
    "    \n",
    "    rbf = Rbf(east, north, value, function='linear')\n",
    "    z_new = rbf(gridX.ravel(), gridY.ravel()).reshape(gridX.shape)\n",
    "    print(key)\n",
    "    plt.pcolor(gridX, gridY, z_new);\n",
    "    plt.plot(east, north, 'o');\n",
    "    plt.xlabel('East'); plt.ylabel('North');\n",
    "    plt.title('RBF Linear interpolation');\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "    transform = Affine.translation(gridX[0][0]-rRes/2, gridY[0][0]-rRes/2)*Affine.scale(rRes,rRes)\n",
    "\n",
    "    new_dataset = rio.open(cwd +'/temp/meteo_rbf_'+key[:(len(key)-4)]+'.tif', 'w', driver='GTiff',\n",
    "                            height = z_new.shape[0], width = z_new.shape[1],\n",
    "                            count=1, dtype=str(z_new.dtype),\n",
    "                            crs='+proj=utm +zone=32 +datum=WGS84 +units=m +no_defs',\n",
    "                            transform=transform)\n",
    "\n",
    "    new_dataset.write(z_new, 1)\n",
    "    new_dataset.close()\n",
    "    #Warp with GDAL since rasterio and rasterstats affine transformation don't work in the grid processing notebook\n",
    "    ds = gdal.Warp(cwd +'/temp/int_'+key[:(len(key)-4)]+'.tif', cwd +'/temp/meteo_rbf_'+key[:(len(key)-4)]+'.tif', dstSRS='EPSG:32632',\n",
    "                   outputType=gdal.GDT_Float64, xRes=rRes, yRes=rRes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e850fbc2",
   "metadata": {},
   "source": [
    "Check if Points Samplings in the interpolated raster corresponds with measured data for each stations positions. The Point Samplings (raster values) histogram is compared with the histogram of the station values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead14be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in aq_dict:\n",
    "    raster = rio.open(cwd +'/temp/int_'+key[:(len(key)-4)]+'.tif')\n",
    "    east = aq_dict[key].geometry.x\n",
    "    north = aq_dict[key].geometry.y\n",
    "    value = aq_dict[key][['valore']]\n",
    "    aq_dict[key].index = range(len(aq_dict[key]))\n",
    "    coords = [(x,y) for x, y in zip(aq_dict[key].geometry.x,aq_dict[key].geometry.y)]\n",
    "    aq_dict[key]['Raster Value'] = [x[0] for x in raster.sample(coords)]\n",
    "    rmse = ((aq_dict[key]['Raster Value'] - aq_dict[key]['valore'])**2)**(1/2)\n",
    "    print(\"Data name : {name}\".format(name = key))\n",
    "    print(\"Max value from stations: {value}\".format(value = aq_dict[key]['valore'].max()))\n",
    "    print(\"Max value from interpolated raster: {value}\".format(value=aq_dict[key]['Raster Value'].max()))\n",
    "    print(\"Min value from stations: {value}\".format(value = aq_dict[key]['valore'].min()))\n",
    "    print(\"Min value from interpolated raster: {value}\".format(value=aq_dict[key]['Raster Value'].min()))\n",
    "    print(\"RMSE max: {rmse}\".format(rmse = rmse.max()))\n",
    "    print(\"RMSE mean : {rmse}\".format(rmse = rmse.mean()))\n",
    "    plt.hist(aq_dict[key]['Raster Value'], label = 'Raster Values', histtype='step')\n",
    "    plt.hist(aq_dict[key]['valore'], label = 'Stations Values', histtype='step')\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.show()\n",
    "    print(\"----------------------------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236f96e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in meteo_dict:\n",
    "    raster = rio.open(cwd +'/temp/int_'+key[:(len(key)-4)]+'.tif')\n",
    "    east = meteo_dict[key].geometry.x\n",
    "    north = meteo_dict[key].geometry.y\n",
    "    value = meteo_dict[key][['valore']]\n",
    "    meteo_dict[key].index = range(len(meteo_dict[key]))\n",
    "    coords = [(x,y) for x, y in zip(meteo_dict[key].geometry.x, meteo_dict[key].geometry.y)]\n",
    "    meteo_dict[key]['Raster Value'] = [x[0] for x in raster.sample(coords)]\n",
    "    rmse = ((meteo_dict[key]['Raster Value'] - meteo_dict[key]['valore'])**2)**(1/2)\n",
    "    print(\"Data name : {name}\".format(name = key))\n",
    "    print(\"Max value from stations: {value}\".format(value = meteo_dict[key]['valore'].max()))\n",
    "    print(\"Max value from interpolated raster: {value}\".format(value=meteo_dict[key]['Raster Value'].max()))\n",
    "    print(\"Min value from stations: {value}\".format(value = meteo_dict[key]['valore'].min()))\n",
    "    print(\"Min value from interpolated raster: {value}\".format(value=meteo_dict[key]['Raster Value'].min()))\n",
    "    print(\"RMSE max: {rmse}\".format(rmse = rmse.max()))\n",
    "    print(\"RMSE mean : {rmse}\".format(rmse = rmse.mean()))\n",
    "    plt.hist(meteo_dict[key]['Raster Value'], label = 'Raster Values', histtype='step')\n",
    "    plt.hist(meteo_dict[key]['valore'], label = 'Stations Values', histtype='step')\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.show()\n",
    "    print(\"----------------------------------\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cddee7c-3778-4b19-af49-2c69175dcd8f",
   "metadata": {},
   "source": [
    "# ESA LPS Air Quality Platform data request notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68de3c0e-c503-4e31-96b8-6ad92148372e",
   "metadata": {},
   "source": [
    "This notebook is used to download data from both meteorological and air quality ground sensor of LPS Air Quality Platform data.\n",
    "\n",
    "The data are retrieved using the API service.\n",
    "The sensors available are: \n",
    "- PM2.5\n",
    "- PM10\n",
    "- NO2\n",
    "- CO2\n",
    "- NH3\n",
    "- CO\n",
    "- Temperature\n",
    "- Humidity\n",
    "\n",
    "The API request downloads the data in the given time range. A Z-Score test is performed to remove outliers and the mean value for each sensor is calculated.\n",
    "Mean values for each sensor are saved in geopackage format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cf36b5-a1e1-45f4-a639-d86ba0aea367",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Reference website: \n",
    "- ESA LPS Air Quality Platform: https://aqp.eo.esa.int/aqstation/\n",
    "- ESA LPS Air Quality Platform sensor map: https://aqp.eo.esa.int/map/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b860b20-99d5-4af2-b73c-d03e2afb92cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_list = [1, 5, 6, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 36, 58]  #list of devices considered to make the request. Check on map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4085cc05-d909-4c79-9984-3ac048b32845",
   "metadata": {},
   "outputs": [],
   "source": [
    "aq_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76c6314-39c6-4990-918e-9df243bef633",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(device_list)):\n",
    "    url = 'https://api.aqp.eo.esa.int/api/device/'+str(device_list[i])+'/csv?start_date='+start_date+'&end_date='+end_date\n",
    "    urlData = requests.get(url).content\n",
    "    rawData = pd.read_csv(io.StringIO(urlData.decode('utf-8')))\n",
    "    aq_data.append(rawData)\n",
    "\n",
    "df_data = pd.concat((pd.DataFrame(data) for data in aq_data), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02b59a8-09f8-45df-84b4-5a04f09508cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_active = list(df_data.device_id.unique())\n",
    "list_active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa6b5cb-f568-49aa-88aa-1da905a94692",
   "metadata": {},
   "outputs": [],
   "source": [
    "aq_data = []\n",
    "del df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992a3325-b62b-420d-9603-519bdb1e4cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(list_active)):\n",
    "    url = 'https://api.aqp.eo.esa.int/api/device/'+str(list_active[i])+'/csv?start_date='+start_date+'&end_date='+end_date\n",
    "    urlData = requests.get(url).content\n",
    "    rawData = pd.read_csv(io.StringIO(urlData.decode('utf-8')))\n",
    "    aq_data.append(rawData)\n",
    "\n",
    "df_data = pd.concat((pd.DataFrame(data) for data in aq_data), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea74862-7566-4159-ae03-17c30241f3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_columns = ['device_id', 'acquisition_date','latitude', \n",
    "           'longitude', 'pm25', 'pm10', 'humidity', 'temperature',\n",
    "          'no2', 'co2', 'nh3', 'co']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddeed67-6e50-41d8-bb80-37e1879fa5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_data[sel_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba9eec6-ba2f-4e3c-87fe-f46e559ec9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb09390-a119-40c4-9a98-10876c4ea7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['acquisition_date'] = pd.to_datetime(df['acquisition_date'], format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8e8eca-e624-4c79-b1b0-35345ab6a61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm25 = df[['device_id','acquisition_date', 'latitude', 'longitude', 'pm25']]\n",
    "pm10 = df[['device_id','acquisition_date', 'latitude', 'longitude', 'pm10']]\n",
    "humidity = df[['device_id','acquisition_date', 'latitude', 'longitude', 'humidity']]\n",
    "temperature = df[['device_id','acquisition_date', 'latitude', 'longitude', 'temperature']]\n",
    "no2 = df[['device_id','acquisition_date', 'latitude', 'longitude', 'no2']]\n",
    "co2 = df[['device_id','acquisition_date', 'latitude', 'longitude', 'co2']]\n",
    "nh3 = df[['device_id','acquisition_date', 'latitude', 'longitude', 'nh3']]\n",
    "co = df[['device_id','acquisition_date', 'latitude', 'longitude', 'co']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ab1723-1148-4b79-a9f5-0e33e7289d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm25 = pm25.rename(columns={'pm25': 'value'})\n",
    "pm10 = pm10.rename(columns={'pm10': 'value'})\n",
    "humidity = humidity.rename(columns={'humidity': 'value'})\n",
    "temperature = temperature.rename(columns={'temperature': 'value'})\n",
    "no2 = no2.rename(columns={'no2': 'value'})\n",
    "co2 = co2.rename(columns={'co2': 'value'})\n",
    "nh3 = nh3.rename(columns={'nh3': 'value'})\n",
    "co = co.rename(columns={'co': 'value'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e27368-3626-47cc-954d-ebeb9243e543",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm25['zscore'] = np.abs(stats.zscore(pm25['value'], nan_policy='omit'))\n",
    "pm25 = pm25[pm25.zscore < threshold]\n",
    "pm25 = pm25.groupby(['device_id'],as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114d893c-596e-4cb0-ad8b-f892a2a621d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm10['zscore'] = np.abs(stats.zscore(pm10['value'], nan_policy='omit'))\n",
    "pm10 = pm10[pm10.zscore < threshold]\n",
    "pm10 = pm10.groupby(['device_id'],as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca49ffba-ea8b-4d01-8825-5996beb62e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "humidity['zscore'] = np.abs(stats.zscore(humidity['value'], nan_policy='omit'))\n",
    "humidity = humidity[humidity.zscore < threshold]\n",
    "humidity = humidity.groupby(['device_id'],as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97d729d-4110-4d7f-8781-8295b8c4a629",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature['zscore'] = np.abs(stats.zscore(temperature['value'], nan_policy='omit'))\n",
    "temperature = temperature[temperature.zscore < threshold]\n",
    "temperature = temperature.groupby(['device_id'],as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437197ef-6010-4bb0-9e35-7b34e1a18a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "no2['zscore'] = np.abs(stats.zscore(no2['value'], nan_policy='omit'))\n",
    "no2 = no2[no2.zscore < threshold]\n",
    "no2 = no2.groupby(['device_id'],as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c4ae39-49df-400e-b178-e3d37a3969d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2['zscore'] = np.abs(stats.zscore(co2['value'], nan_policy='omit'))\n",
    "co2 = co2[co2.zscore < threshold]\n",
    "co2 = co2.groupby(['device_id'],as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab42df10-0b6a-4d0e-8f0d-56af766eef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "nh3['zscore'] = np.abs(stats.zscore(nh3['value'], nan_policy='omit'))\n",
    "nh3 = nh3[nh3.zscore < threshold]\n",
    "nh3 = nh3.groupby(['device_id'],as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58e36ae-5e52-4b8e-8ce4-7021af1c7846",
   "metadata": {},
   "outputs": [],
   "source": [
    "co['zscore'] = np.abs(stats.zscore(co['value'], nan_policy='omit'))\n",
    "co = co[co.zscore < threshold]\n",
    "co = co.groupby(['device_id'],as_index=False).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1067910e-192f-4d1f-a8bd-b21e76caf884",
   "metadata": {},
   "source": [
    "Create Geodataframe and save in geopackage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d36ca8-e3e9-42df-9a6e-d81d90e83cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm25_gdf = gpd.GeoDataFrame(pm25, geometry=gpd.points_from_xy(pm25.longitude, pm25.latitude)).set_crs('epsg:4326')\n",
    "pm10_gdf = gpd.GeoDataFrame(pm10, geometry=gpd.points_from_xy(pm10.longitude, pm10.latitude)).set_crs('epsg:4326')\n",
    "hum_gdf = gpd.GeoDataFrame(humidity, geometry=gpd.points_from_xy(humidity.longitude, humidity.latitude)).set_crs('epsg:4326')\n",
    "temp_gdf = gpd.GeoDataFrame(temperature, geometry=gpd.points_from_xy(temperature.longitude, temperature.latitude)).set_crs('epsg:4326')\n",
    "no2_gdf = gpd.GeoDataFrame(no2, geometry=gpd.points_from_xy(no2.longitude, no2.latitude)).set_crs('epsg:4326')\n",
    "co2_gdf = gpd.GeoDataFrame(co2, geometry=gpd.points_from_xy(co2.longitude, co2.latitude)).set_crs('epsg:4326')\n",
    "nh3_gdf = gpd.GeoDataFrame(nh3, geometry=gpd.points_from_xy(nh3.longitude, nh3.latitude)).set_crs('epsg:4326')\n",
    "co_gdf = gpd.GeoDataFrame(co, geometry=gpd.points_from_xy(co.longitude, co.latitude)).set_crs('epsg:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581e6754-bdf8-43a8-adaf-2241e62cbf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm25_gdf.to_file(cwd+\"/temp/pm25_lcs.gpkg\", driver=\"GPKG\")\n",
    "pm10_gdf.to_file(cwd+\"/temp/pm10_lcs.gpkg\", driver=\"GPKG\")\n",
    "hum_gdf.to_file(cwd+\"/temp/hum_lcs.gpkg\", driver=\"GPKG\")\n",
    "temp_gdf.to_file(cwd+\"/temp/temp_lcs.gpkg\", driver=\"GPKG\")\n",
    "no2_gdf.to_file(cwd+\"/temp/no2_lcs.gpkg\", driver=\"GPKG\")\n",
    "co2_gdf.to_file(cwd+\"/temp/co2_lcs.gpkg\", driver=\"GPKG\")\n",
    "nh3_gdf.to_file(cwd+\"/temp/nh3_lcs.gpkg\", driver=\"GPKG\")\n",
    "co_gdf.to_file(cwd+\"/temp/co_lcs.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc18c0da-fd88-4b1a-8cf8-433d737fbb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm10_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486f57bd-1520-484f-9a3b-6b37846c1584",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnd = gpd.read_file('C:/Users/Administrator/OneDrive - Politecnico di Milano/WP2/D-DUST/boundaries/lombardy_region2020.gpkg').to_crs('epsg:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4da60a4-c0c2-4519-9983-f764ce8f646b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "bnd.plot(ax = ax);\n",
    "hum_gdf.plot(ax=ax,marker='o', color='red', markersize=20, edgecolors='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5244ac4-117c-4657-9292-3db7fd35070d",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981248c0-0398-48b7-95ac-b616aba79a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code used for testing kriging interpolation (left as example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ef2d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pykrige.ok import OrdinaryKriging\n",
    "# from pykrige.kriging_tools import write_asc_grid\n",
    "# import pykrige.kriging_tools as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab607b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in aq_dict:\n",
    "#     east = aq_dict[key].geometry.x\n",
    "#     north = aq_dict[key].geometry.y\n",
    "#     value = aq_dict[key][['valore']]\n",
    "#     n_sensor = aq_dict[key].shape[0]  #VERIFICARE nlags in funzione del numero di sensori (defaul 6)\n",
    "#     lag = int(math.sqrt(n_sensor))\n",
    "    \n",
    "#     OK = OrdinaryKriging(east, north,value, weight=True,nlags= lag ,variogram_model='power',coordinates_type='euclidean') #verbose=True, enable_plotting=False, weight=w\n",
    "#     # print(UniversalKriging.__doc__ )\n",
    "#     gridded, ss1 = OK.execute('grid', xRange, yRange)\n",
    "#     transform = Affine.translation(gridX[0][0]-rRes/2, gridY[0][0]-rRes/2)*Affine.scale(rRes,rRes)\n",
    "#     rasterCrs = CRS.from_epsg(32632)\n",
    "#     #definition, register and close of interpolated raster\n",
    "#     interpRaster = rio.open(cwd +'/temp/aq_krig_'+key[:(len(key)-4)]+'.tif',\n",
    "#                                     'w',\n",
    "#                                     driver='GTiff',\n",
    "#                                     height=gridded.shape[0],\n",
    "#                                     width=gridded.shape[1],\n",
    "#                                     count=1,\n",
    "#                                     nodata = -9999,\n",
    "#                                     dtype=gridded.dtype,\n",
    "#                                     crs=rasterCrs,\n",
    "#                                     transform=transform,\n",
    "#                                     )\n",
    "#     interpRaster.write(gridded, 1)\n",
    "#     interpRaster.close()\n",
    "#     #Warp with GDAL since rasterio and rasterstats affine transformation don't work in the grid processing notebook\n",
    "#     ds = gdal.Warp(cwd +'/temp/int_'+key[:(len(key)-4)]+'.tif', cwd +'/temp/aq_krig_'+key[:(len(key)-4)]+'.tif', dstSRS='EPSG:32632',\n",
    "#                outputType=gdal.GDT_Float64, xRes=rRes, yRes=rRes)\n",
    "#     print(key)\n",
    "#     plt.imshow(gridded)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453e6c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in meteo_dict:\n",
    "#     east = meteo_dict[key].geometry.x\n",
    "#     north = meteo_dict[key].geometry.y\n",
    "#     value = meteo_dict[key][['valore']]\n",
    "#     n_sensor = meteo_dict[key].shape[0]  #VERIFICARE nlags in funzione del numero di sensori (defaul 6)\n",
    "#     lag = int(math.sqrt(n_sensor))\n",
    "    \n",
    "#     OK = OrdinaryKriging(east, north, value, nlags= lag,\n",
    "#                  weight=True, variogram_model='power',coordinates_type='euclidean') #verbose=True, enable_plotting=False, weight=w\n",
    "#     # print(OrdinaryKriging.__doc__)\n",
    "#     gridded, ss1 = OK.execute('grid', xRange, yRange)\n",
    "#     transform = Affine.translation(gridX[0][0]-rRes/2, gridY[0][0]-rRes/2)*Affine.scale(rRes,rRes)\n",
    "#     rasterCrs = CRS.from_epsg(32632)\n",
    "#     #definition, register and close of interpolated raster\n",
    "#     interpRaster = rio.open(cwd +'/temp/meteo_krig_'+key[:(len(key)-4)]+'.tif',\n",
    "#                                     'w',\n",
    "#                                     driver='GTiff',\n",
    "#                                     height= gridded.shape[0],\n",
    "#                                     width= gridded.shape[1],\n",
    "#                                     count=1,\n",
    "#                                     nodata = -9999,\n",
    "#                                     dtype=gridded.dtype,\n",
    "#                                     crs=rasterCrs,\n",
    "#                                     transform=transform,\n",
    "#                                     )\n",
    "#     interpRaster.write(gridded, 1)\n",
    "#     interpRaster.close()\n",
    "#     #Warp with GDAL since rasterio and rasterstats affine transformation don't work in the grid processing notebook\n",
    "#     ds = gdal.Warp(cwd +'/temp/int_'+key[:(len(key)-4)]+'.tif', cwd +'/temp/meteo_krig_'+key[:(len(key)-4)]+'.tif', dstSRS='EPSG:32632',\n",
    "#                outputType=gdal.GDT_Float64, xRes=rRes, yRes=rRes)\n",
    "#     print(key)\n",
    "#     print(lag)\n",
    "#     plt.imshow(gridded)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd721f2d",
   "metadata": {},
   "source": [
    "Notes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4277434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTERPOLATION USING GDAL\n",
    "# pts = ogr.Open(cwd+'/temp/air_hum_st.gpkg', update = True)\n",
    "# layer=pts.GetLayer()\n",
    "\n",
    "# to generate a an interpolation using GDAL library\n",
    "# pts = layer = None\n",
    "# idw = gdal.Grid(\"idw.tif\", (cwd+'/temp/air_hum_st.gpkg'), zfield=\"valore\",\n",
    "#                algorithm = \"invdist\", outputBounds = [ulx,uly,lrx,lry],\n",
    "#                width = xsize, height = ysize)\n",
    "# idw = None\n",
    "\n",
    "# points = list(zip(air_hum_st.lng,air_hum_st.lat))\n",
    "\n",
    "# gridded = griddata(points, value, (gridX,gridY), method='linear',fill_value=0)\n",
    "\n",
    "# gridded = gridded.reshape((gridded.shape[0], gridded.shape[1]))\n",
    "\n",
    "# plt.imshow(gridded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc802025",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mask data using date range\n",
    "#mask = (meteo_data['data'] >= start_date) & (meteo_data['data'] < end_date)\n",
    "#meteo_data = meteo_data.loc[mask]\n",
    "#meteo_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc21b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list(arpa_df.columns))\n",
    "# print(arpa_df['idsensore'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c82fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = client.get_all(dati, idsensore = \"100\", data='2022-01-20')\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e05945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arpa_df.loc[arpa_df['idsensore'] == \"10377\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec9c574",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
