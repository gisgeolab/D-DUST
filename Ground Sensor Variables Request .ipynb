{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e1c351c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Ground Sensors Variables Request Notebook\n",
    "\n",
    "<center><img src=img/DDUST__Nero.png width=\"300\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d050786",
   "metadata": {},
   "source": [
    "This notebook is used to download data from both meteorological and air quality ground sensors of the ARPA Lombardia network segment. In this introduction, these data are described to provide a clearer overview for the two datasets.\n",
    "\n",
    "In detail, each sensor type, position, and time series are retrieved as follows:\n",
    "- The ground sensor's type and position are always retrieved through the ARPA API. They are available at the following links: [Air quality stations](https://www.dati.lombardia.it/Ambiente/Stazioni-qualit-dell-aria/ib47-atvt) and [Meteorological stations](https://www.dati.lombardia.it/Ambiente/Stazioni-Meteorologiche/nf78-nj6b)\n",
    "- Time series are available through API requests for the **current year only** (e.g. from January 2022 if the current year is 2022) for [Air quality data](https://www.dati.lombardia.it/Ambiente/Dati-sensori-aria/nicp-bhqi), while they are available for the **current month** for  [Meteorological data](https://www.dati.lombardia.it/Ambiente/Dati-sensori-meteo/647i-nhxk) (this may change in the future)\n",
    "- To use data from previous years it's required to use the dataset in .csv format, such as [Air quality data for 2020]( https://www.dati.lombardia.it/Ambiente/Dati-sensori-aria-2020/88sp-5tmj) or [Meteorological data for 2020](https://www.dati.lombardia.it/Ambiente/Dati-sensori-meteo-2020/erjn-istm). It is possible to download the .zip folder containing these .csv data using the `DDUST_methods.py` contained inside the  `functions` folder. The links for downloading these data are contained inside the `AQ_sensor` and the `meteo_sensor` functions. One .zip folder for each year is available.\n",
    "\n",
    "In this notebook, the zipped file is downloaded only if it's not already available in the working directory, otherwise the download is skipped.\n",
    "\n",
    "In order to change the date range it is necessary to modify the time range in the `date.json` file and the right data will be automatically considered. This allows to have the same date selected across the D-DUST Jupyter notebooks.\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<span>&#9888;</span>\n",
    "<a id='warning'></a> Note: at the end of the notebook there is a function where ground sensor data interpolation is performed using Radial Basis Function (RBF) method (from Scipy (https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.Rbf.html) library) over the region of interest, in order to get continuous information of the air quality and meteorological data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613745f5",
   "metadata": {},
   "source": [
    "### Reference:\n",
    "ARPA data from API are accessible using [Socrata Open Data API](https://dev.socrata.com/). <br>\n",
    "\n",
    "Register on \"Open Data Lombardia\" to get the API token: https://www.dati.lombardia.it/login\n",
    "\n",
    "The library [sodapy](https://github.com/xmunoz/sodapy) is a python client for the Socrata Open Data API.\n",
    "The `app_token` is required to access the data. The token can be copied and pasted into the `keys.json` file (A sample of the `keys.json` file is provided in the repository)  or a `.env` file can be used in the working directory. It's just required to replace your token with those provided by ARPA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43a976f",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feaa305",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sodapy import Socrata\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "import json\n",
    "import io\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Set current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Import functions defined for DDUST project:\n",
    "from functions import DDUST_methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489fe353",
   "metadata": {},
   "source": [
    "You can modify the `date.json` file to change the date range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4ef9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = open('date.json')  # Import time range from the date.json file\n",
    "date = json.load(d)\n",
    "year = date['year']\n",
    "custom_week = date['custom_week']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa424104-8b8a-447e-9f43-963c055098fe",
   "metadata": {},
   "source": [
    "Select start and end dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e8acbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select start and end date of the corresponding selected week:\n",
    "start_date = datetime.datetime.strptime((str(year)+'-'+custom_week[0]), \"%Y-%m-%d\").date()\n",
    "end_date = datetime.datetime.strptime((str(year)+'-'+custom_week[1]), \"%Y-%m-%d\").date()\n",
    "print(\"The starting date is\", start_date,\"and the ending date is\" , end_date,\". The date is define as yyyy-mm-dd.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701fbd8d-d8ca-4434-8d37-ae46c2fa1441",
   "metadata": {},
   "source": [
    "Prepare dates formatted properly and read the keys/tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b394f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract start, end dates and selected year for ARPA sensor. This is done to consider also the last day in the computation, otherwise it's skipped\n",
    "start_date_dt = datetime.datetime.strptime((str(year)+'-'+custom_week[0]), '%Y-%m-%d')\n",
    "end_date_dt = datetime.datetime.strptime((str(year)+'-'+custom_week[1]), '%Y-%m-%d')+timedelta(days=1) #increase 1 day to select data from arpa sensor correctly\n",
    "start_date = str(start_date_dt)[0:10]\n",
    "end_date = str(end_date_dt)[0:10]\n",
    "year = datetime.datetime.strptime(start_date, '%Y-%m-%d').date().year\n",
    "\n",
    "# Key and app token for Socrata API\n",
    "f = open('keys.json')\n",
    "keys = json.load(f)\n",
    "\n",
    "#Z-Score threshold for removing outliers (at the end of each section)\n",
    "threshold = 4  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7646d207-3d76-41d7-a41a-ad15d0e68b52",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a2ef3a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import meteorological stations information and positions from ARPA API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f837e094",
   "metadata": {},
   "source": [
    "This part will only request meteorological sensor information/metadata, types and positions from the API (there are no times series):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8f1ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "arpa_domain = \"www.dati.lombardia.it\"  # Select domain\n",
    "m_st_descr = \"nf78-nj6b\" # Select dataset\n",
    "client = Socrata(arpa_domain, app_token = keys['arpa_token'])  #Initialize client\n",
    "results = client.get_all(m_st_descr)\n",
    "meteo_st_descr = pd.DataFrame(results)\n",
    "meteo_st_descr[\"idsensore\"] = meteo_st_descr[\"idsensore\"].astype(int)\n",
    "meteo_st_descr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e2542b-ab92-4fa3-a70e-d43db154d444",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0363cc",
   "metadata": {},
   "source": [
    "# Import Meteorological data from ARPA API (current month data) or .csv file (past years)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f5524c-7141-4ded-b533-3fa0eb6e6032",
   "metadata": {},
   "source": [
    "The next code is used to request data from API (**only current month for meteorological data**) or download the .zip folder containing the .csv files (past years data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a62881e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If current month is selected, use data from API (only current months data ara available from meteo sensors)\n",
    "if int(year) == datetime.datetime.today().year:\n",
    "    \n",
    "    # Set domain and token \n",
    "    arpa_domain = \"www.dati.lombardia.it\"\n",
    "    dati = \"647i-nhxk\" #change this depending on the dataset (check Open Data Lombardia datasets)\n",
    "    \n",
    "    # Query the data\n",
    "    client = Socrata(arpa_domain, app_token = keys['arpa_token']) #insert your arpa_token\n",
    "    date_query = \"data > {} and data < {}\".format('\"'+ start_date + '\"','\"'+ end_date + '\"')\n",
    "    results = client.get(dati, where=date_query, limit=5000000000000)\n",
    "    \n",
    "    # Create the dataframe\n",
    "    meteo_data = pd.DataFrame(results)\n",
    "    meteo_data.rename(columns={'IdSensore': 'idsensore','Data': 'data','idOperatore': 'idoperatore','Stato': 'stato','Valore': 'valore'}, inplace=True)\n",
    "    meteo_data['data'] =  pd.to_datetime(meteo_data['data'], format='%Y/%m/%d %H:%M:%S')\n",
    "    meteo_data_df = meteo_data.astype({\"idsensore\": int,\"valore\": float})\n",
    "    \n",
    "# If data from previous years are requested then download the corresponding .zip file, extract the .csv file and filter the dates  \n",
    "elif int(year) < datetime.datetime.today().year:\n",
    "    filename_meteo = 'meteo_'+str(year)+'.zip'\n",
    "    \n",
    "    #if file does not exist then download it\n",
    "    if not os.path.exists(os.path.join(filename_meteo)):\n",
    "        csv_url = my_methods.meteo_sensor(str(year))\n",
    "        r_meteo = requests.get(csv_url, allow_redirects=True)\n",
    "        DL_zip = open(filename, 'wb').write(r_meteo.content)\n",
    "        print('Dowloaded zip file')\n",
    "    \n",
    "    print('Zip file exist')\n",
    "    \n",
    "    # Open the zip file\n",
    "    archive = zipfile.ZipFile(filename_meteo, 'r')\n",
    "    data = archive.open(str(year)+'.csv') \n",
    "    \n",
    "    # Create dataframe\n",
    "    # Keep meteo_data_df in memory so is possible to use it for other time periods\n",
    "    meteo_data_df = pd.read_csv(data, dtype={\"IdSensore\": int,\"Valore\": float, \"Stato\": str, \"idOperatore\":str})\n",
    "    meteo_data_df.rename(columns={'IdSensore': 'idsensore','Data': 'data','idOperatore': 'idoperatore','Stato': 'stato','Valore': 'valore'}, inplace=True)\n",
    "    meteo_data_df['data'] =  pd.to_datetime(meteo_data_df['data'], format='%d/%m/%Y %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e209aab3-0892-410d-81a5-0c6c0c77e909",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mask meteo_data_df\n",
    "mask = (meteo_data_df.data >= start_date) & (meteo_data_df.data < end_date)\n",
    "meteo_data = meteo_data_df.loc[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7896ee3a",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b0b80f",
   "metadata": {},
   "source": [
    "# Meteorological data processing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799f6280",
   "metadata": {},
   "source": [
    "Drop `stato`, `idoperatore` columns and select valid data different from -9999:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbd41e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_data = meteo_data.drop(columns=['stato', 'idoperatore'])\n",
    "meteo_data = meteo_data[meteo_data.valore.astype(float) != -9999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fe7643-41e8-456c-9a10-7f89d77fbc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the dataframe\n",
    "meteo_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7666d5",
   "metadata": {},
   "source": [
    "Get sensors unique types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cb27bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_st_descr.tipologia.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba773355",
   "metadata": {},
   "source": [
    "Select relevant sensors by adding them to the following list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c37e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_sensor_sel = ['Precipitazione','Temperatura','Umidità Relativa','Direzione Vento','Velocità Vento', 'Radiazione Globale']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95403cea",
   "metadata": {},
   "source": [
    "Join sensors data with their descriptions and information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c9d40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_table = pd.merge(meteo_data, meteo_st_descr, on = 'idsensore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b27716",
   "metadata": {},
   "source": [
    "Filter sensor by type using the `m_sensor_list`, checking if their type is in that list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91118ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_table['tipologia'].astype(str)\n",
    "meteo_table = meteo_table[meteo_table['tipologia'].isin(m_sensor_sel)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1d26c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_st = meteo_table.loc[meteo_table['tipologia'] == 'Temperatura']\n",
    "prec_st = meteo_table.loc[meteo_table['tipologia'] == 'Precipitazione']\n",
    "air_hum_st = meteo_table.loc[meteo_table['tipologia'] == 'Umidità Relativa']\n",
    "wind_dir_st = meteo_table.loc[meteo_table['tipologia'] == 'Direzione Vento']\n",
    "wind_speed_st = meteo_table.loc[meteo_table['tipologia'] == 'Velocità Vento']\n",
    "rad_glob_st = meteo_table.loc[meteo_table['tipologia'] == 'Radiazione Globale']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8494dcc-b648-46b2-acb4-35e0734fd80f",
   "metadata": {},
   "source": [
    "Since Wind Direction's values are expressed in Degrees North, the are divided into 8 categories:\n",
    " - `1` -> North: 0° - 22.5° / 337.5° - 360°\n",
    " - `2` -> North-East: 22.5° - 67.5°         \n",
    " - `3` -> East: 67.5° - 112.5°              \n",
    " - `4` -> South-East: 112.5° - 157.5°       \n",
    " - `5` -> South: 15.5° - 202.5°             \n",
    " - `6` -> South-West: 202.5° - 247.5°       \n",
    " - `7` -> West: 247.5° - 292.5°             \n",
    " - `8` -> North-West: 292.5° - 337.5°       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18062c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_dir_st = wind_dir_st.rename(columns={\"valore\": \"direzione\"})\n",
    "wind_dir_st.loc[(wind_dir_st.direzione >= 0 ) & (wind_dir_st.direzione < 22.5 ), 'valore'] = 1\n",
    "wind_dir_st.loc[(wind_dir_st.direzione > 337.5 ) & (wind_dir_st.direzione <= 360 ), 'valore'] = 1\n",
    "wind_dir_st.loc[(wind_dir_st.direzione >= 22.5 ) & (wind_dir_st.direzione < 67.5 ), 'valore'] = 2\n",
    "wind_dir_st.loc[(wind_dir_st.direzione >= 67.5 ) & (wind_dir_st.direzione < 112.5 ), 'valore'] = 3\n",
    "wind_dir_st.loc[(wind_dir_st.direzione >= 112.5 ) & (wind_dir_st.direzione < 157.5 ), 'valore'] = 4\n",
    "wind_dir_st.loc[(wind_dir_st.direzione >= 157.5 ) & (wind_dir_st.direzione < 202.5 ), 'valore'] = 5\n",
    "wind_dir_st.loc[(wind_dir_st.direzione >= 202.5 ) & (wind_dir_st.direzione < 247.5 ), 'valore'] = 6\n",
    "wind_dir_st.loc[(wind_dir_st.direzione >= 247.5 ) & (wind_dir_st.direzione < 292.5 ), 'valore'] = 7\n",
    "wind_dir_st.loc[(wind_dir_st.direzione >= 292.5 ) & (wind_dir_st.direzione <= 337.5 ), 'valore'] = 8\n",
    "wind_dir_st = wind_dir_st.groupby('idsensore')['valore'].apply(lambda x: x.mode().iat[0]).reset_index()\n",
    "wind_dir_st = pd.merge(wind_dir_st, meteo_st_descr, on = 'idsensore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda9dd1b-4512-4029-b94f-37d28b7990ee",
   "metadata": {},
   "source": [
    "To remove outliers from `precipitation` we consider a threshold of 100 mm/h (the values coming from the station are stored every 10 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a8ffa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# precipitation values less than 100 to remove outliers\n",
    "prec_st = prec_st[prec_st.valore < 100]\n",
    "prec_st = prec_st.groupby(['idsensore'],as_index=False).mean()\n",
    "prec_st = pd.merge(prec_st, meteo_st_descr, on = 'idsensore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a88f6ec",
   "metadata": {},
   "source": [
    "To remove outliers from other variables we can calculate the corresponding Z-Scores  and calculate mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbacd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_st['zscore'] = np.abs(stats.zscore(temp_st['valore'], nan_policy='propagate'))\n",
    "temp_st = temp_st[temp_st.zscore < threshold]\n",
    "temp_st = temp_st.groupby(['idsensore'],as_index=False).mean()\n",
    "temp_st = pd.merge(temp_st, meteo_st_descr, on = 'idsensore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82ab7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_hum_st['zscore'] = np.abs(stats.zscore(air_hum_st['valore'], nan_policy='propagate'))\n",
    "air_hum_st = air_hum_st[air_hum_st.zscore < threshold]\n",
    "air_hum_st = air_hum_st.groupby(['idsensore'],as_index=False).mean()\n",
    "air_hum_st = pd.merge(air_hum_st, meteo_st_descr, on = 'idsensore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6033ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rad_glob_st['zscore'] = np.abs(stats.zscore(rad_glob_st['valore'], nan_policy='propagate'))\n",
    "rad_glob_st = rad_glob_st[rad_glob_st.zscore < threshold]\n",
    "rad_glob_st = rad_glob_st.groupby(['idsensore'],as_index=False).mean()\n",
    "rad_glob_st = pd.merge(rad_glob_st, meteo_st_descr, on = 'idsensore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50e2dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_speed_st['zscore'] = np.abs(stats.zscore(wind_speed_st['valore'], nan_policy='propagate'))\n",
    "wind_speed_st = wind_speed_st[wind_speed_st.zscore < threshold]\n",
    "wind_speed_st = wind_speed_st.groupby(['idsensore'],as_index=False).mean()\n",
    "wind_speed_st = pd.merge(wind_speed_st, meteo_st_descr, on = 'idsensore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb40326d",
   "metadata": {},
   "source": [
    "Save sensors separately and create a `.gpkg` file for each one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879efd88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp_gdf = gpd.GeoDataFrame(temp_st, geometry=gpd.points_from_xy(temp_st.lng, temp_st.lat))\n",
    "temp_gdf = temp_gdf.set_crs('epsg:4326')\n",
    "prec_gdf = gpd.GeoDataFrame(prec_st, geometry=gpd.points_from_xy(prec_st.lng, prec_st.lat))\n",
    "prec_gdf = prec_gdf.set_crs('epsg:4326')\n",
    "air_hum_gdf = gpd.GeoDataFrame(air_hum_st, geometry=gpd.points_from_xy(air_hum_st.lng, air_hum_st.lat))\n",
    "air_hum_gdf = air_hum_gdf.set_crs('epsg:4326')\n",
    "wind_dir_gdf = gpd.GeoDataFrame(wind_dir_st, geometry=gpd.points_from_xy(wind_dir_st.lng, wind_dir_st.lat))\n",
    "wind_dir_gdf = wind_dir_gdf.set_crs('epsg:4326')\n",
    "wind_speed_gdf = gpd.GeoDataFrame(wind_speed_st, geometry=gpd.points_from_xy(wind_speed_st.lng, wind_speed_st.lat))\n",
    "wind_speed_gdf = wind_speed_gdf.set_crs('epsg:4326')\n",
    "rad_glob_gdf = gpd.GeoDataFrame(rad_glob_st, geometry=gpd.points_from_xy(rad_glob_st.lng, rad_glob_st.lat))\n",
    "rad_glob_gdf = rad_glob_gdf.set_crs('epsg:4326')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bee154c-7c6c-4d9a-af87-e4c00215aace",
   "metadata": {},
   "source": [
    "Save in the temporary `temp` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7280ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_gdf.to_file(cwd+\"/temp/temp_st.gpkg\", driver=\"GPKG\")\n",
    "prec_gdf.to_file(cwd+\"/temp/prec_st.gpkg\", driver=\"GPKG\")\n",
    "air_hum_gdf.to_file(cwd+\"/temp/air_hum_st.gpkg\", driver=\"GPKG\")\n",
    "wind_dir_gdf.to_file(cwd+\"/temp/wind_dir_st.gpkg\", driver=\"GPKG\")\n",
    "wind_speed_gdf.to_file(cwd+\"/temp/wind_speed_st.gpkg\", driver=\"GPKG\")\n",
    "rad_glob_gdf.to_file(cwd+\"/temp/rad_glob_st.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0f3b09",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc69c0f",
   "metadata": {},
   "source": [
    "# Import Air Quality stations information and positions from ARPA API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234d6fe9",
   "metadata": {},
   "source": [
    "This sections has the same structure of the previous one. First, import air quality sensors descriptions/metadata, typs and positions from the API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada35c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "arpa_domain = \"www.dati.lombardia.it\"\n",
    "st_descr = \"ib47-atvt\"\n",
    "\n",
    "client = Socrata(arpa_domain, app_token = keys['arpa_token']) \n",
    "results = client.get_all(st_descr)\n",
    "\n",
    "air_st_descr = pd.DataFrame(results)\n",
    "air_st_descr[\"idsensore\"] = air_st_descr[\"idsensore\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cebc2c0",
   "metadata": {},
   "source": [
    "- - - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e463d807",
   "metadata": {},
   "source": [
    "# Import Air Quality data from ARPA API (current year data) or .csv file (past years)\n",
    "\n",
    "For Air Quality data, they are available through the API for the current year. For older data the .csv file must be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6bfcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If current year is selected use data from API\n",
    "if int(year) == datetime.datetime.today().year:\n",
    "    \n",
    "    # Set domain and token \n",
    "    arpa_domain = \"www.dati.lombardia.it\"\n",
    "    dati = \"nicp-bhqi\" #change this depending on the dataset (check Open Data Lombardia datasets)\n",
    "    client = Socrata(arpa_domain, app_token = keys['arpa_token'])\n",
    "    \n",
    "    # Query the data\n",
    "    date_query = \"data > {} and data < {}\".format('\"'+ start_date + '\"','\"'+ end_date + '\"')  #select date \n",
    "    results = client.get(dati, where=date_query, limit=5000000000)  # query\n",
    "    \n",
    "    # Create the dataframe\n",
    "    aq_data = pd.DataFrame(results) #create Dataframe\n",
    "    aq_data.rename(columns={'IdSensore': 'idsensore','Data': 'data','idOperatore': 'idoperatore','Stato': 'stato','Valore': 'valore'}, inplace=True)\n",
    "    aq_data['data'] =  pd.to_datetime(aq_data['data'], format='%Y/%m/%d %H:%M:%S')\n",
    "    aq_data_df = aq_data.astype({\"idsensore\": int,\"valore\": float})\n",
    "    \n",
    "# If data from previous years are requested then download the corresponding .zip file, extract the .csv file and filter the dates  \n",
    "elif int(year) < datetime.datetime.today().year:\n",
    "    filename_aq = 'aq_'+str(year)+'.zip'\n",
    "    \n",
    "    #if file does not exist then download it\n",
    "    if not os.path.exists(os.path.join(filename_aq)):\n",
    "        csv_url = DDUST_methods.AQ_sensor(str(year))\n",
    "        r_aq = requests.get(csv_url, allow_redirects=True)\n",
    "        DL_zip = open(filename_aq, 'wb').write(r_aq.content)\n",
    "        print('Dowloaded zip file')\n",
    "    \n",
    "    print('Zip file exist')\n",
    "    \n",
    "    # Open the zip file\n",
    "    archive = zipfile.ZipFile('aq_'+str(year)+'.zip', 'r')\n",
    "    data = archive.open(str(year)+'.csv') #open the zip\n",
    "    \n",
    "    # Create dataframe\n",
    "    aq_data_df=pd.read_csv(data)\n",
    "    aq_data_df.rename(columns={'IdSensore': 'idsensore','Data': 'data','idOperatore': 'idoperatore','Stato': 'stato','Valore': 'valore'}, inplace=True)\n",
    "    aq_data_df['data'] =  pd.to_datetime(aq_data_df['data'], format='%d/%m/%Y %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f89d15-fa09-4a99-aa6b-d6a338e8deb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (aq_data_df.data >= start_date) & (aq_data_df.data < end_date)\n",
    "aq_data = aq_data_df.loc[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1b1012",
   "metadata": {},
   "source": [
    "- - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe9e011",
   "metadata": {},
   "source": [
    "# Air quality data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9f68f9",
   "metadata": {},
   "source": [
    "Drop \"stato\" and \"idoperatore\" columns and select valid values different from -9999:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec42686",
   "metadata": {},
   "outputs": [],
   "source": [
    "aq_data = aq_data.drop(columns=['stato', 'idoperatore'])\n",
    "aq_data = aq_data[aq_data.valore.astype(float) != -9999]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66af98ca",
   "metadata": {},
   "source": [
    "Get the unique sensor type names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76ad095",
   "metadata": {},
   "outputs": [],
   "source": [
    "air_st_descr.nometiposensore.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20ff6bc-8cd8-4e8a-81ca-c65e5c0c94a5",
   "metadata": {},
   "source": [
    "Select relevant sensors and put them in the following list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd53a61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_sel = ['Ossidi di Azoto', 'Monossido di Carbonio', 'Biossido di Azoto','Ozono',\n",
    "       'Biossido di Zolfo', 'Particelle sospese PM2.5','Ammoniaca','PM10 (SM2005)']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed5f022",
   "metadata": {},
   "source": [
    "Join sensors description and information with the mean value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081fd777",
   "metadata": {},
   "outputs": [],
   "source": [
    "aq_table = pd.merge(aq_data, air_st_descr, on='idsensore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b38e5b",
   "metadata": {},
   "source": [
    "Select sensors adding their names to the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4d1d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "aq_table['nometiposensore'].astype(str)\n",
    "aq_table = aq_table[aq_table['nometiposensore'].isin(sensor_sel)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a9cde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm25_st = aq_table.loc[aq_table['nometiposensore'] == 'Particelle sospese PM2.5']\n",
    "co_st = aq_table.loc[aq_table['nometiposensore'] == 'Monossido di Carbonio']\n",
    "no2_st = aq_table.loc[aq_table['nometiposensore'] == 'Biossido di Azoto']\n",
    "so2_st = aq_table.loc[aq_table['nometiposensore'] == 'Biossido di Zolfo']\n",
    "nh3_st = aq_table.loc[aq_table['nometiposensore'] == 'Ammoniaca']\n",
    "nox_st = aq_table.loc[aq_table['nometiposensore'] == 'Ossidi di Azoto']\n",
    "pm10_st = aq_table.loc[aq_table['nometiposensore'] == 'PM10 (SM2005)']\n",
    "o3_st = aq_table.loc[aq_table['nometiposensore'] == 'Ozono']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3e87c3",
   "metadata": {},
   "source": [
    "Z-Test to remove outliers and calculate mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f1f03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm25_st['zscore'] = np.abs(stats.zscore(pm25_st['valore'], nan_policy='propagate'))\n",
    "pm25_st = pm25_st[pm25_st.zscore < threshold]\n",
    "pm25_st = pm25_st.groupby(['idsensore'],as_index=False).mean()\n",
    "pm25_st = pd.merge(pm25_st, air_st_descr, on='idsensore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d74657",
   "metadata": {},
   "outputs": [],
   "source": [
    "co_st['zscore'] = np.abs(stats.zscore(co_st['valore'], nan_policy='propagate'))\n",
    "co_st = co_st[co_st.zscore < threshold]\n",
    "co_st = co_st.groupby(['idsensore'],as_index=False).mean()\n",
    "co_st = pd.merge(co_st, air_st_descr, on='idsensore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0fd8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "no2_st['zscore'] = np.abs(stats.zscore(no2_st['valore'], nan_policy='propagate'))\n",
    "no2_st = no2_st[no2_st.zscore < threshold]\n",
    "no2_st = no2_st.groupby(['idsensore'],as_index=False).mean()\n",
    "no2_st = pd.merge(no2_st, air_st_descr, on='idsensore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e77d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "so2_st['zscore'] = np.abs(stats.zscore(so2_st['valore'], nan_policy='propagate'))\n",
    "so2_st = so2_st[so2_st.zscore < threshold]\n",
    "so2_st = so2_st.groupby(['idsensore'],as_index=False).mean()\n",
    "so2_st = pd.merge(so2_st, air_st_descr, on='idsensore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da90f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "nh3_st['zscore'] = np.abs(stats.zscore(nh3_st['valore'], nan_policy='propagate'))\n",
    "nh3_st = nh3_st[nh3_st.zscore < threshold]\n",
    "nh3_st = nh3_st.groupby(['idsensore'],as_index=False).mean()\n",
    "nh3_st = pd.merge(nh3_st, air_st_descr, on='idsensore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240c4e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "nox_st['zscore'] = np.abs(stats.zscore(nox_st['valore'], nan_policy='propagate'))\n",
    "nox_st = nox_st[nox_st.zscore < threshold]\n",
    "nox_st = nox_st.groupby(['idsensore'],as_index=False).mean()\n",
    "nox_st = pd.merge(nox_st, air_st_descr, on='idsensore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b1b010",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm10_st['zscore'] = np.abs(stats.zscore(pm10_st['valore'], nan_policy='propagate'))\n",
    "pm10_st = pm10_st[pm10_st.zscore < threshold]\n",
    "pm10_st = pm10_st.groupby(['idsensore'],as_index=False).mean()\n",
    "pm10_st = pd.merge(pm10_st, air_st_descr, on='idsensore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2500d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3_st['zscore'] = np.abs(stats.zscore(o3_st['valore'], nan_policy='propagate'))\n",
    "o3_st = o3_st[o3_st.zscore < threshold]\n",
    "o3_st = o3_st.groupby(['idsensore'],as_index=False).mean()\n",
    "o3_st = pd.merge(o3_st, air_st_descr, on='idsensore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f305ba7c",
   "metadata": {},
   "source": [
    "Save sensors separately and create a .gpkg file for each one. They contain the mean value for that given variable calculated over the defined time range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c1a145",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm25_gdf = gpd.GeoDataFrame(pm25_st, geometry=gpd.points_from_xy(pm25_st.lng, pm25_st.lat))\n",
    "pm25_gdf = pm25_gdf.set_crs('epsg:4326')\n",
    "co_gdf = gpd.GeoDataFrame(co_st, geometry=gpd.points_from_xy(co_st.lng, co_st.lat))\n",
    "co_gdf = co_gdf.set_crs('epsg:4326')\n",
    "no2_gdf = gpd.GeoDataFrame(no2_st, geometry=gpd.points_from_xy(no2_st.lng, no2_st.lat))\n",
    "no2_gdf = no2_gdf.set_crs('epsg:4326')\n",
    "so2_gdf = gpd.GeoDataFrame(so2_st, geometry=gpd.points_from_xy(so2_st.lng, so2_st.lat))\n",
    "so2_gdf = so2_gdf.set_crs('epsg:4326')\n",
    "nh3_gdf = gpd.GeoDataFrame(nh3_st, geometry=gpd.points_from_xy(nh3_st.lng, nh3_st.lat))\n",
    "nh3_gdf = nh3_gdf.set_crs('epsg:4326')\n",
    "nox_gdf = gpd.GeoDataFrame(nox_st, geometry=gpd.points_from_xy(nox_st.lng, nox_st.lat))\n",
    "nox_gdf = nox_gdf.set_crs('epsg:4326')\n",
    "pm10_gdf = gpd.GeoDataFrame(pm10_st, geometry=gpd.points_from_xy(pm10_st.lng, pm10_st.lat))\n",
    "pm10_gdf = pm10_gdf.set_crs('epsg:4326')\n",
    "o3_gdf = gpd.GeoDataFrame(o3_st, geometry=gpd.points_from_xy(o3_st.lng, o3_st.lat))\n",
    "o3_gdf = o3_gdf.set_crs('epsg:4326')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d21aca7",
   "metadata": {},
   "source": [
    "Save in a **temp** folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b31e1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm25_gdf.to_file(cwd+\"/temp/pm25_st.gpkg\", driver=\"GPKG\")\n",
    "co_gdf.to_file(cwd+\"/temp/co_st.gpkg\", driver=\"GPKG\")\n",
    "no2_gdf.to_file(cwd+\"/temp/no2_st.gpkg\", driver=\"GPKG\")\n",
    "so2_gdf.to_file(cwd+\"/temp/so2_st.gpkg\", driver=\"GPKG\")\n",
    "nh3_gdf.to_file(cwd+\"/temp/nh3_st.gpkg\", driver=\"GPKG\")\n",
    "nox_gdf.to_file(cwd+\"/temp/nox_st.gpkg\", driver=\"GPKG\")\n",
    "pm10_gdf.to_file(cwd+\"/temp/pm10_st.gpkg\", driver=\"GPKG\")\n",
    "o3_gdf.to_file(cwd+\"/temp/o3_st.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9410221c-7f99-4082-8181-3f2208144be3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eb6f3c",
   "metadata": {},
   "source": [
    "# Sensor interpolation using Radial Basis Functions (RBF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991423c7-f4f3-4e95-a425-fac0ff52ff5e",
   "metadata": {},
   "source": [
    "ARPA sensors are interpolated using Scipy Radial Basis Functions (RBF) (this is not dont for ESA Air Quality Platforms they are few)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650b7616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal\n",
    "from osgeo import ogr\n",
    "import rasterio as rio\n",
    "from rasterio.transform import Affine\n",
    "from rasterio.crs import CRS\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import Rbf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bcca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm25_gdf = pm25_gdf.to_crs('epsg:32632')\n",
    "co_gdf = co_gdf.to_crs('epsg:32632')\n",
    "no2_gdf = no2_gdf.to_crs('epsg:32632')\n",
    "so2_gdf = so2_gdf.to_crs('epsg:32632')\n",
    "nh3_gdf = nh3_gdf.to_crs('epsg:32632')\n",
    "nox_gdf = nox_gdf.to_crs('epsg:32632')\n",
    "pm10_gdf = pm10_gdf.to_crs('epsg:32632')\n",
    "o3_gdf = o3_gdf.to_crs('epsg:32632')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cf1ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_gdf = temp_gdf.to_crs('epsg:32632')\n",
    "prec_gdf = prec_gdf.to_crs('epsg:32632')\n",
    "air_hum_gdf = air_hum_gdf.to_crs('epsg:32632')\n",
    "wind_dir_gdf = wind_dir_gdf.to_crs('epsg:32632')\n",
    "wind_speed_gdf = wind_speed_gdf.to_crs('epsg:32632')\n",
    "rad_glob_gdf = rad_glob_gdf.to_crs('epsg:32632')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab7b8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = gdal.Open(cwd+\"/bounding_box/bounding_box_buffer20_raster_32632.tif\")  #use a predefined raster to set dimensions\n",
    "gt = bb.GetGeoTransform()\n",
    "ulx = gt[0]\n",
    "uly = gt[3]\n",
    "resx = gt[1]\n",
    "resy = gt[5]\n",
    "xsize = bb.RasterXSize\n",
    "ysize = bb.RasterYSize\n",
    "lrx = ulx + xsize * resx\n",
    "lry = uly + ysize * resy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d0a8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "xsize, ysize  #image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f68fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ulx, uly #upper left coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3f6ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrx, lry #lower right coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c5d618-fa28-49fd-ad3f-aea05b86cc2a",
   "metadata": {},
   "source": [
    "Define the output desired spatial resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37b0a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "rRes = 250  #Spatial resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69121f18-375a-4441-b83c-a0148a6fa1bc",
   "metadata": {},
   "source": [
    "Create a meshgrid with given dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed678fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "xRange = np.arange(ulx,lrx+rRes,rRes)\n",
    "yRange = np.arange(lry,uly+rRes,rRes)\n",
    "gridX,gridY = np.meshgrid(xRange, yRange)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1a62a5-8c97-4edf-b7d8-15558dad26da",
   "metadata": {},
   "source": [
    "Create dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56130a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "aq_dict = {'pm25_gdf':pm25_gdf, 'co_gdf':co_gdf, 'no2_gdf':no2_gdf, 'so2_gdf':so2_gdf,\n",
    "           'nh3_gdf':nh3_gdf, 'nox_gdf':nox_gdf, 'pm10_gdf':pm10_gdf, 'o3_gdf':o3_gdf}\n",
    "\n",
    "meteo_dict = {'temp_gdf':temp_gdf, 'prec_gdf':prec_gdf, 'air_hum_gdf':air_hum_gdf,\n",
    "           'wind_speed_gdf':wind_speed_gdf, 'rad_glob_gdf':rad_glob_gdf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da672f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in aq_dict:\n",
    "    east = aq_dict[key].geometry.x\n",
    "    north = aq_dict[key].geometry.y\n",
    "    value = aq_dict[key][['valore']]\n",
    "    \n",
    "    rbf = Rbf(east, north, value, function='linear')\n",
    "    z_new = rbf(gridX.ravel(), gridY.ravel()).reshape(gridX.shape)\n",
    "    print(key)\n",
    "    plt.pcolor(gridX, gridY, z_new);\n",
    "    plt.plot(east, north, 'o', markerfacecolor='blue');\n",
    "    plt.xlabel('East'); plt.ylabel('North');\n",
    "    plt.title('RBF Linear interpolation');\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "    transform = Affine.translation(gridX[0][0]-rRes/2, gridY[0][0]-rRes/2)*Affine.scale(rRes,rRes)\n",
    "\n",
    "    new_dataset = rio.open(cwd +'/temp/aq_rbf_'+key[:(len(key)-4)]+'.tif', 'w', driver='GTiff',\n",
    "                            height = z_new.shape[0], width = z_new.shape[1],\n",
    "                            count=1, dtype=str(z_new.dtype),\n",
    "                            crs='+proj=utm +zone=32 +datum=WGS84 +units=m +no_defs',\n",
    "                            transform=transform)\n",
    "\n",
    "    new_dataset.write(z_new, 1)\n",
    "    new_dataset.close()\n",
    "    #Warp with GDAL since rasterio and rasterstats affine transformation don't work in the grid processing notebook\n",
    "    ds = gdal.Warp(cwd +'/temp/int_'+key[:(len(key)-4)]+'.tif', cwd +'/temp/aq_rbf_'+key[:(len(key)-4)]+'.tif', dstSRS='EPSG:32632',\n",
    "                   outputType=gdal.GDT_Float64, xRes=rRes, yRes=rRes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc72b5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in meteo_dict:\n",
    "    east = meteo_dict[key].geometry.x\n",
    "    north = meteo_dict[key].geometry.y\n",
    "    value = meteo_dict[key][['valore']]\n",
    "    \n",
    "    rbf = Rbf(east, north, value, function='linear')\n",
    "    z_new = rbf(gridX.ravel(), gridY.ravel()).reshape(gridX.shape)\n",
    "    print(key)\n",
    "    plt.pcolor(gridX, gridY, z_new);\n",
    "    plt.plot(east, north, 'o');\n",
    "    plt.xlabel('East'); plt.ylabel('North');\n",
    "    plt.title('RBF Linear interpolation');\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "    transform = Affine.translation(gridX[0][0]-rRes/2, gridY[0][0]-rRes/2)*Affine.scale(rRes,rRes)\n",
    "\n",
    "    new_dataset = rio.open(cwd +'/temp/meteo_rbf_'+key[:(len(key)-4)]+'.tif', 'w', driver='GTiff',\n",
    "                            height = z_new.shape[0], width = z_new.shape[1],\n",
    "                            count=1, dtype=str(z_new.dtype),\n",
    "                            crs='+proj=utm +zone=32 +datum=WGS84 +units=m +no_defs',\n",
    "                            transform=transform)\n",
    "\n",
    "    new_dataset.write(z_new, 1)\n",
    "    new_dataset.close()\n",
    "    #Warp with GDAL since rasterio and rasterstats affine transformation don't work in the grid processing notebook\n",
    "    ds = gdal.Warp(cwd +'/temp/int_'+key[:(len(key)-4)]+'.tif', cwd +'/temp/meteo_rbf_'+key[:(len(key)-4)]+'.tif', dstSRS='EPSG:32632',\n",
    "                   outputType=gdal.GDT_Float64, xRes=rRes, yRes=rRes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e850fbc2",
   "metadata": {},
   "source": [
    "Check if Points Samplings in the interpolated raster corresponds with measured data for each stations positions. The Point Samplings (raster values obtained from interpolation) histogram is compared with the histogram of the station values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead14be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in aq_dict:\n",
    "    raster = rio.open(cwd +'/temp/int_'+key[:(len(key)-4)]+'.tif')\n",
    "    east = aq_dict[key].geometry.x\n",
    "    north = aq_dict[key].geometry.y\n",
    "    value = aq_dict[key][['valore']]\n",
    "    aq_dict[key].index = range(len(aq_dict[key]))\n",
    "    coords = [(x,y) for x, y in zip(aq_dict[key].geometry.x,aq_dict[key].geometry.y)]\n",
    "    n = len(coords)\n",
    "    aq_dict[key]['Raster Value'] = [x[0] for x in raster.sample(coords)]\n",
    "    rmse = ((np.sum((aq_dict[key]['Raster Value'] - aq_dict[key]['valore']))**2)/n)**(1/2)\n",
    "    print(n)\n",
    "    print(\"Data name : {name}\".format(name = key))\n",
    "    print(\"Max value from stations: {value}\".format(value = aq_dict[key]['valore'].max()))\n",
    "    print(\"Max value from interpolated raster: {value}\".format(value=aq_dict[key]['Raster Value'].max()))\n",
    "    print(\"Min value from stations: {value}\".format(value = aq_dict[key]['valore'].min()))\n",
    "    print(\"Min value from interpolated raster: {value}\".format(value=aq_dict[key]['Raster Value'].min()))\n",
    "    print(\"RMSE:\", rmse)\n",
    "    plt.hist(aq_dict[key]['Raster Value'], label = 'Raster Values', histtype='step')\n",
    "    plt.hist(aq_dict[key]['valore'], label = 'Stations Values', histtype='step')\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.show()\n",
    "    print(\"----------------------------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236f96e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in meteo_dict:\n",
    "    raster = rio.open(cwd +'/temp/int_'+key[:(len(key)-4)]+'.tif')\n",
    "    east = meteo_dict[key].geometry.x\n",
    "    north = meteo_dict[key].geometry.y\n",
    "    value = meteo_dict[key][['valore']]\n",
    "    meteo_dict[key].index = range(len(meteo_dict[key]))\n",
    "    coords = [(x,y) for x, y in zip(meteo_dict[key].geometry.x, meteo_dict[key].geometry.y)]\n",
    "    n = len(coords)\n",
    "    meteo_dict[key]['Raster Value'] = [x[0] for x in raster.sample(coords)]\n",
    "    rmse = ((np.sum((meteo_dict[key]['Raster Value'] - meteo_dict[key]['valore']))**2)/n)**(1/2)\n",
    "    print(n)\n",
    "    print(\"Data name : {name}\".format(name = key))\n",
    "    print(\"Max value from stations: {value}\".format(value = meteo_dict[key]['valore'].max()))\n",
    "    print(\"Max value from interpolated raster: {value}\".format(value=meteo_dict[key]['Raster Value'].max()))\n",
    "    print(\"Min value from stations: {value}\".format(value = meteo_dict[key]['valore'].min()))\n",
    "    print(\"Min value from interpolated raster: {value}\".format(value=meteo_dict[key]['Raster Value'].min()))\n",
    "    print(\"RMSE:\", rmse)\n",
    "    plt.hist(meteo_dict[key]['Raster Value'], label = 'Raster Values', histtype='step')\n",
    "    plt.hist(meteo_dict[key]['valore'], label = 'Stations Values', histtype='step')\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.show()\n",
    "    print(\"----------------------------------\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cddee7c-3778-4b19-af49-2c69175dcd8f",
   "metadata": {},
   "source": [
    "# ESA LPS Air Quality Platform data request notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68de3c0e-c503-4e31-96b8-6ad92148372e",
   "metadata": {},
   "source": [
    "This notebook is used to download data from both meteorological and air quality ground sensor of LPS Air Quality Platform data.\n",
    "\n",
    "The data are retrieved using the API service.\n",
    "The sensors available are: \n",
    "- PM2.5\n",
    "- PM10\n",
    "- NO2\n",
    "- CO2\n",
    "- NH3\n",
    "- CO\n",
    "- Temperature\n",
    "- Humidity\n",
    "\n",
    "The API request downloads the data in the given time range. A Z-Score test is performed to remove outliers and the mean value for each sensor is calculated.\n",
    "Mean values for each sensor are saved in geopackage format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cf36b5-a1e1-45f4-a639-d86ba0aea367",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Reference website: \n",
    "- ESA LPS Air Quality Platform: https://aqp.eo.esa.int/aqstation/\n",
    "- ESA LPS Air Quality Platform sensor map: https://aqp.eo.esa.int/map/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b860b20-99d5-4af2-b73c-d03e2afb92cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_list = [1, 5, 6, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 36, 58]  #list of devices considered to make the request. Check on map on the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4085cc05-d909-4c79-9984-3ac048b32845",
   "metadata": {},
   "outputs": [],
   "source": [
    "aq_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76c6314-39c6-4990-918e-9df243bef633",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(device_list)):\n",
    "    url = 'https://api.aqp.eo.esa.int/api/device/'+str(device_list[i])+'/csv?start_date='+start_date+'&end_date='+end_date\n",
    "    urlData = requests.get(url).content\n",
    "    rawData = pd.read_csv(io.StringIO(urlData.decode('utf-8')))\n",
    "    aq_data.append(rawData)\n",
    "\n",
    "df_data = pd.concat((pd.DataFrame(data) for data in aq_data), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02b59a8-09f8-45df-84b4-5a04f09508cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_active = list(df_data.device_id.unique())\n",
    "list_active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa6b5cb-f568-49aa-88aa-1da905a94692",
   "metadata": {},
   "outputs": [],
   "source": [
    "aq_data = []\n",
    "del df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992a3325-b62b-420d-9603-519bdb1e4cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(list_active)):\n",
    "    url = 'https://api.aqp.eo.esa.int/api/device/'+str(list_active[i])+'/csv?start_date='+start_date+'&end_date='+end_date\n",
    "    urlData = requests.get(url).content\n",
    "    rawData = pd.read_csv(io.StringIO(urlData.decode('utf-8')))\n",
    "    aq_data.append(rawData)\n",
    "\n",
    "df_data = pd.concat((pd.DataFrame(data) for data in aq_data), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea74862-7566-4159-ae03-17c30241f3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_columns = ['device_id', 'acquisition_date','latitude', \n",
    "           'longitude', 'pm25', 'pm10', 'humidity', 'temperature',\n",
    "          'no2', 'co2', 'nh3', 'co']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddeed67-6e50-41d8-bb80-37e1879fa5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_data[sel_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba9eec6-ba2f-4e3c-87fe-f46e559ec9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb09390-a119-40c4-9a98-10876c4ea7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['acquisition_date'] = pd.to_datetime(df['acquisition_date'], format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8e8eca-e624-4c79-b1b0-35345ab6a61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm25 = df[['device_id','acquisition_date', 'latitude', 'longitude', 'pm25']]\n",
    "pm10 = df[['device_id','acquisition_date', 'latitude', 'longitude', 'pm10']]\n",
    "humidity = df[['device_id','acquisition_date', 'latitude', 'longitude', 'humidity']]\n",
    "temperature = df[['device_id','acquisition_date', 'latitude', 'longitude', 'temperature']]\n",
    "no2 = df[['device_id','acquisition_date', 'latitude', 'longitude', 'no2']]\n",
    "co2 = df[['device_id','acquisition_date', 'latitude', 'longitude', 'co2']]\n",
    "nh3 = df[['device_id','acquisition_date', 'latitude', 'longitude', 'nh3']]\n",
    "co = df[['device_id','acquisition_date', 'latitude', 'longitude', 'co']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ab1723-1148-4b79-a9f5-0e33e7289d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm25 = pm25.rename(columns={'pm25': 'value'})\n",
    "pm10 = pm10.rename(columns={'pm10': 'value'})\n",
    "humidity = humidity.rename(columns={'humidity': 'value'})\n",
    "temperature = temperature.rename(columns={'temperature': 'value'})\n",
    "no2 = no2.rename(columns={'no2': 'value'})\n",
    "co2 = co2.rename(columns={'co2': 'value'})\n",
    "nh3 = nh3.rename(columns={'nh3': 'value'})\n",
    "co = co.rename(columns={'co': 'value'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e27368-3626-47cc-954d-ebeb9243e543",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm25['zscore'] = np.abs(stats.zscore(pm25['value'], nan_policy='omit'))\n",
    "pm25 = pm25[pm25.zscore < threshold]\n",
    "pm25 = pm25.groupby(['device_id'],as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114d893c-596e-4cb0-ad8b-f892a2a621d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm10['zscore'] = np.abs(stats.zscore(pm10['value'], nan_policy='omit'))\n",
    "pm10 = pm10[pm10.zscore < threshold]\n",
    "pm10 = pm10.groupby(['device_id'],as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca49ffba-ea8b-4d01-8825-5996beb62e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "humidity['zscore'] = np.abs(stats.zscore(humidity['value'], nan_policy='omit'))\n",
    "humidity = humidity[humidity.zscore < threshold]\n",
    "humidity = humidity.groupby(['device_id'],as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97d729d-4110-4d7f-8781-8295b8c4a629",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature['zscore'] = np.abs(stats.zscore(temperature['value'], nan_policy='omit'))\n",
    "temperature = temperature[temperature.zscore < threshold]\n",
    "temperature = temperature.groupby(['device_id'],as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437197ef-6010-4bb0-9e35-7b34e1a18a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "no2['zscore'] = np.abs(stats.zscore(no2['value'], nan_policy='omit'))\n",
    "no2 = no2[no2.zscore < threshold]\n",
    "no2 = no2.groupby(['device_id'],as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c4ae39-49df-400e-b178-e3d37a3969d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "co2['zscore'] = np.abs(stats.zscore(co2['value'], nan_policy='omit'))\n",
    "co2 = co2[co2.zscore < threshold]\n",
    "co2 = co2.groupby(['device_id'],as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab42df10-0b6a-4d0e-8f0d-56af766eef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "nh3['zscore'] = np.abs(stats.zscore(nh3['value'], nan_policy='omit'))\n",
    "nh3 = nh3[nh3.zscore < threshold]\n",
    "nh3 = nh3.groupby(['device_id'],as_index=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58e36ae-5e52-4b8e-8ce4-7021af1c7846",
   "metadata": {},
   "outputs": [],
   "source": [
    "co['zscore'] = np.abs(stats.zscore(co['value'], nan_policy='omit'))\n",
    "co = co[co.zscore < threshold]\n",
    "co = co.groupby(['device_id'],as_index=False).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1067910e-192f-4d1f-a8bd-b21e76caf884",
   "metadata": {},
   "source": [
    "Create Geodataframe and save in geopackage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d36ca8-e3e9-42df-9a6e-d81d90e83cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm25_gdf = gpd.GeoDataFrame(pm25, geometry=gpd.points_from_xy(pm25.longitude, pm25.latitude)).set_crs('epsg:4326')\n",
    "pm10_gdf = gpd.GeoDataFrame(pm10, geometry=gpd.points_from_xy(pm10.longitude, pm10.latitude)).set_crs('epsg:4326')\n",
    "hum_gdf = gpd.GeoDataFrame(humidity, geometry=gpd.points_from_xy(humidity.longitude, humidity.latitude)).set_crs('epsg:4326')\n",
    "temp_gdf = gpd.GeoDataFrame(temperature, geometry=gpd.points_from_xy(temperature.longitude, temperature.latitude)).set_crs('epsg:4326')\n",
    "no2_gdf = gpd.GeoDataFrame(no2, geometry=gpd.points_from_xy(no2.longitude, no2.latitude)).set_crs('epsg:4326')\n",
    "co2_gdf = gpd.GeoDataFrame(co2, geometry=gpd.points_from_xy(co2.longitude, co2.latitude)).set_crs('epsg:4326')\n",
    "nh3_gdf = gpd.GeoDataFrame(nh3, geometry=gpd.points_from_xy(nh3.longitude, nh3.latitude)).set_crs('epsg:4326')\n",
    "co_gdf = gpd.GeoDataFrame(co, geometry=gpd.points_from_xy(co.longitude, co.latitude)).set_crs('epsg:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581e6754-bdf8-43a8-adaf-2241e62cbf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm25_gdf.to_file(cwd+\"/temp/pm25_lcs.gpkg\", driver=\"GPKG\")\n",
    "pm10_gdf.to_file(cwd+\"/temp/pm10_lcs.gpkg\", driver=\"GPKG\")\n",
    "hum_gdf.to_file(cwd+\"/temp/hum_lcs.gpkg\", driver=\"GPKG\")\n",
    "temp_gdf.to_file(cwd+\"/temp/temp_lcs.gpkg\", driver=\"GPKG\")\n",
    "no2_gdf.to_file(cwd+\"/temp/no2_lcs.gpkg\", driver=\"GPKG\")\n",
    "co2_gdf.to_file(cwd+\"/temp/co2_lcs.gpkg\", driver=\"GPKG\")\n",
    "nh3_gdf.to_file(cwd+\"/temp/nh3_lcs.gpkg\", driver=\"GPKG\")\n",
    "co_gdf.to_file(cwd+\"/temp/co_lcs.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc18c0da-fd88-4b1a-8cf8-433d737fbb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm10_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486f57bd-1520-484f-9a3b-6b37846c1584",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnd = gpd.read_file('C:/Users/Administrator/OneDrive - Politecnico di Milano/WP2/D-DUST/boundaries/lombardy_region2020.gpkg').to_crs('epsg:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4da60a4-c0c2-4519-9983-f764ce8f646b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "bnd.plot(ax = ax);\n",
    "hum_gdf.plot(ax=ax,marker='o', color='red', markersize=20, edgecolors='black')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
